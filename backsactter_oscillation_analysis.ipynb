{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the experiments for naming of the files\n",
    "experiment_list = [\"E007\", \"E008\", \"E011\"]\n",
    "\n",
    "# list of the names of the excel files with the raw data of the experiments\n",
    "file_name_list = [\"..\\\\data\\\\\" + i for i in os.listdir(\"..\\\\data\") if i.endswith(\".xlsx\")]\n",
    "file_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion factor from inches to cm for the size of the plots\n",
    "cm_in_inches = 2.54\n",
    "width = 21\n",
    "height = 27.6\n",
    "dimensions1 = (width/cm_in_inches, height/cm_in_inches)\n",
    "dimensions2 = (width/cm_in_inches, height/2/cm_in_inches)\n",
    "lw = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with the colors associated with the strains contained in the experiments\n",
    "color_dic = {r\"WT\": \"forestgreen\",\n",
    "             r\"KaiA3\": \"brown\",\n",
    "             r\"KaiB3\": \"crimson\",\n",
    "             r\"KaiC3\": \"darkorange\",\n",
    "             r\"KaiA3B3\": \"olive\",\n",
    "             r\"KaiA3C3\": \"lawngreen\",\n",
    "             r\"KaiB3C3\": \"mediumturquoise\",\n",
    "             r\"KaiA3B3C3\": \"dodgerblue\",\n",
    "             r\"KaiA1B1C1\": \"orchid\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renaming_of_raw_data(file_name_list:list, experiments:list, save=False) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    This function reads in the raw data as a pandas DataFrame and renames the\n",
    "    columns based on the names given to each well in the BioLection software.\n",
    "    The data is then trimmed to the first 84 h and unnecessary strains are removed\n",
    "    and the result is saved to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    for file, experiment in zip(file_name_list, experiments):\n",
    "        # raw data is saved in sheet 4 of the excel file\n",
    "        raw_data = pd.read_excel(file, sheet_name=3, skiprows=range(1,4))\n",
    "        keys = raw_data.keys()[2:]\n",
    "        keys_dict = {i: re.sub(r\"Ch\\d_\", r\"\", j) for i, j in zip(keys, keys)}\n",
    "        raw_data = raw_data.rename(columns=keys_dict)\n",
    "        raw_data = raw_data[[i for i in raw_data.keys() if i.startswith(\"raw\") or i.startswith(\"Time[h]\")]]\n",
    "\n",
    "\n",
    "        my_names = [i.replace(\"raw_\", \"\") for i in raw_data.keys()]\n",
    "        # the names are saved in sheet 2 of the excel file\n",
    "        names = pd.read_excel(file, sheet_name=1, skiprows=range(1,4))\n",
    "        names = names.query(\"Well == @my_names\")\n",
    "        names_dict = {\"raw_\" + i: j.replace(\",\", \"\").replace(\" \", \"_\").replace(\"Uppsala_\", \"\") for i, j in zip(names[\"Well\"], names[\"Description\"])}\n",
    "        raw_data = raw_data.rename(columns=names_dict)\n",
    "\n",
    "        # trimming the data to the first 84 h\n",
    "        index_end = raw_data[raw_data[\"Time[h]\"] <= 84].shape[0] + 1 \n",
    "        raw_data = raw_data.iloc[:index_end,]   \n",
    "\n",
    "        # trimming strains that are not needed:\n",
    "        raw_data = raw_data[[key for key in raw_data.keys() if not re.search(r\"7942|glgC|mVenus|Wilde\", key)]]\n",
    "\n",
    "        # saving the result to a csv file:\n",
    "        if save:\n",
    "            raw_data.to_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renaming_of_raw_data(file_name_list, experiment_list, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_unique_keys(experiment_list:list) -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for identifying the unique part of the names of the DataFrame\n",
    "    and saving them in a dictionary sorted by the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    experiments_2_key_2_names = {}\n",
    "    for experiment in experiment_list:\n",
    "        data = pd.read_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\", index_col=0)\n",
    "        unique_keys = {}\n",
    "        for i in data.keys()[1:]:\n",
    "            unique_part = \"_\".join(i.split(\"_\")[0:2])\n",
    "            if unique_part not in unique_keys:\n",
    "                unique_keys[unique_part] = []\n",
    "            unique_keys[unique_part].append(i)\n",
    "\n",
    "        experiments_2_key_2_names[experiment] = unique_keys\n",
    "    return experiments_2_key_2_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_keys = identify_unique_keys(experiment_list=experiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(y:pd.Series, kernel:np.array, poly=4) -> (np.array, np.array, np.array):\n",
    "\n",
    "    \"\"\"\n",
    "    Take a pd.Series from the raw data and fit a polynomial regression of the 4th degree to it.\n",
    "    The smoothed signal, signal and polynomial regression are returned. Function adapted from\n",
    "    Berwanger et al. 2023 (https://doi.org/10.1101/2023.09.26.559469).\n",
    "    \"\"\"\n",
    "    # 4th degree polynomial regression\n",
    "    y_reg = np.array(np.arange(len(y))).reshape(-1,1)\n",
    "    pf = PolynomialFeatures(poly)\n",
    "    y_reg = pf.fit_transform(y_reg)\n",
    "    reg_fit = LinearRegression().fit(y_reg, y)\n",
    "    reg_predict = reg_fit.predict(y_reg)\n",
    "\n",
    "    # the polynomial regression is subtracted from the raw data\n",
    "    signal = y - reg_predict\n",
    "    # from the result, the mean is subtracted\n",
    "    signal -= signal.mean()\n",
    "    #the signal is smoothened by a rolling average with the kernel size given to the function\n",
    "    smoothed_signal = np.convolve(signal, kernel, mode='same')\n",
    "    return smoothed_signal, signal, reg_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_first_data(experiment_list:list, experiment_keys_keys:dict) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for calculating the average of all replicates of one experiment\n",
    "    and performing the regression analysis on it afterwards.\n",
    "    The results are saved to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    for experiment in experiment_list:\n",
    "\n",
    "        # reading in the trimmed and renamed data\n",
    "        raw_data = pd.read_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\", index_col=0)\n",
    "        unique_keys = experiment_keys[experiment]\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        data[\"Time(h)\"] = raw_data[\"Time[h]\"]\n",
    "\n",
    "        kernel_size = 40\n",
    "        kernel = np.ones(kernel_size) / kernel_size\n",
    "\n",
    "        # the first three hours are cut off to eliminate measuring artifacts\n",
    "        index_1 = data[\"Time(h)\"][data[\"Time(h)\"] >= 3].index[0]\n",
    "\n",
    "        for key in unique_keys:\n",
    "            keys = unique_keys[key]\n",
    "            sub_data = raw_data[keys]\n",
    "\n",
    "            for i in sub_data.keys():\n",
    "                data[i] = sub_data[i]\n",
    "\n",
    "            mean = np.mean(sub_data, axis=1)\n",
    "            std = np.std(sub_data, axis=1)\n",
    "\n",
    "            mean_name = key + \"_mean\"\n",
    "            sd_name = key + \"_sd\"\n",
    "            reg_name = key + \"_reg\"\n",
    "            sig_name = key + \"_signal\"\n",
    "            smooth_name = key + \"_smoothed\"\n",
    "\n",
    "            data[mean_name] = mean\n",
    "            data[sd_name] = std\n",
    "\n",
    "            # regression analysis\n",
    "            smoothed_signal, signal, reg_predict = polynomial_regression(data[mean_name][index_1:], kernel)\n",
    "\n",
    "            nan_arr = np.array([np.nan for i in range(0, data.shape[0] - smoothed_signal.shape[0])])\n",
    "            smoothed_signal = pd.DataFrame(np.concatenate([nan_arr, smoothed_signal], axis=0), columns=[smooth_name])\n",
    "            signal = pd.DataFrame(np.concatenate([nan_arr, signal], axis=0), columns=[sig_name])\n",
    "            reg_predict = pd.DataFrame(np.concatenate([nan_arr, reg_predict], axis=0), columns=[reg_name])\n",
    "\n",
    "            data = pd.concat([data, reg_predict, signal, smoothed_signal], axis=1, ignore_index=False)\n",
    "        data.to_csv(f\"..\\\\data\\\\{experiment}_total_data_mean_first.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_first_data(experiment_list, experiment_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_last_data(experiment_list:list, experiment_keys_keys:dict) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for performing the regression analysis for each replicate first\n",
    "    and then taking the average of the resulting smoothed signal.\n",
    "    The results are saved to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    for experiment in experiment_list:\n",
    "        \n",
    "        # reading in the trimmed data set\n",
    "        raw_data = pd.read_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\", index_col=0)\n",
    "        unique_keys = experiment_keys[experiment]\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        data[\"Time(h)\"] = raw_data[\"Time[h]\"]\n",
    "\n",
    "        kernel_size = 40\n",
    "        kernel = np.ones(kernel_size) / kernel_size\n",
    "        \n",
    "        # the first three hours are cut off to eliminate measuring artifacts\n",
    "        index_1 = data[\"Time(h)\"][data[\"Time(h)\"] >= 3].index[0]\n",
    "\n",
    "        cutoff = data.shape[0] - data[index_1:].shape[0]\n",
    "\n",
    "        for key in unique_keys:\n",
    "            keys = unique_keys[key]\n",
    "            sub_data = raw_data[keys]\n",
    "\n",
    "            for i in sub_data.keys():\n",
    "                data[i] = sub_data[i]\n",
    "\n",
    "                reg_name = i + \"_reg\"\n",
    "                sig_name = i + \"_signal\"\n",
    "                smooth_name = i + \"_smoothed\"\n",
    "\n",
    "                smoothed_signal, signal, reg_predict = polynomial_regression(data[i][index_1:], kernel)\n",
    "\n",
    "                nan_arr = np.array([np.nan for i in range(0, cutoff)])\n",
    "                smoothed_signal = pd.DataFrame(np.concatenate([nan_arr, smoothed_signal], axis=0), columns=[smooth_name])\n",
    "                signal = pd.DataFrame(np.concatenate([nan_arr, signal], axis=0), columns=[sig_name])\n",
    "                reg_predict = pd.DataFrame(np.concatenate([nan_arr, reg_predict], axis=0), columns=[reg_name])\n",
    "                data = pd.concat([data, reg_predict, signal, smoothed_signal], axis=1, ignore_index=False)\n",
    "\n",
    "            smoothed_data = data[[j for j in data.keys() if re.search(r\"smoothed\", j) and re.search(key + \"_\", j)]][cutoff:]\n",
    "\n",
    "            mean_name = key + \"_mean\"\n",
    "            sd_name = key + \"_sd\"\n",
    "\n",
    "            mean = np.mean(smoothed_data, axis=1)\n",
    "            std = np.std(smoothed_data, axis=1)\n",
    "\n",
    "            mean = pd.DataFrame(np.concatenate([nan_arr, mean], axis=0), columns=[mean_name])\n",
    "            std = pd.DataFrame(np.concatenate([nan_arr, std], axis=0), columns=[sd_name])\n",
    "\n",
    "            data[mean_name] = mean\n",
    "            data[sd_name] = std\n",
    "\n",
    "        data.to_csv(f\"..\\\\data\\\\{experiment}_total_data_mean_after.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_last_data(experiment_list, experiment_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_timepoint_csv(experiment_list:list) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for identifying the peaks in the smoothed signal.\n",
    "    The timepoints and heights of the peaks for each strain are saved to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    for experiment in experiment_list:\n",
    "        data = pd.read_csv(f\"..\\\\data\\\\{experiment}_total_data_mean_after.csv\", index_col=0)\n",
    "        sub_data = data[[key for key in data.keys() if re.search(r\"rep\\d_smoothed|Time\", key)]]\n",
    "        peaks_df = pd.DataFrame()\n",
    "        for key in sub_data.iloc[:,1:]:\n",
    "            mean = np.mean(sub_data[key])\n",
    "            maximum = np.max(sub_data[key])\n",
    "\n",
    "            # identifying of the peaks:\n",
    "            #peaks need to be separated by 150 indices, have a width of at least 65 indices and a height of at least the mean + 20 % of the highest value\n",
    "            peaks = find_peaks(sub_data[key], distance=150, width=65, height=mean+0.2*maximum) # 150 <=> 12.5 h; 65 <=> 5.42 h\n",
    "            peaks = peaks[0]\n",
    "            peak_timepoints = np.array([])\n",
    "            peak_heights = np.array([])\n",
    "            name = \"{}_{}_{}\".format(key.split(\"_\")[0], key.split(\"_\")[1], key.split(\"_\")[2])\n",
    "            for peak in peaks:\n",
    "                # only peaks after 12 h are considered\n",
    "                if sub_data[\"Time(h)\"][peak] >= 12:\n",
    "                    peak_timepoints = np.append(peak_timepoints, np.array(sub_data[\"Time(h)\"][peak]))\n",
    "                    peak_heights = np.append(peak_heights, np.array(sub_data[key][peak]))\n",
    "                else:\n",
    "                    continue\n",
    "            # arrays with less than 3 peak timepoints/heights are filled with np.nan so that the results can be put into a DataFrame            \n",
    "            if peak_timepoints.shape[0] < 3:\n",
    "                for i in range(0, 3 - peak_timepoints.shape[0]):\n",
    "                    peak_timepoints = np.append(peak_timepoints, np.nan)\n",
    "                    peak_heights = np.append(peak_heights, np.nan)\n",
    "            peaks_df[name + \"_peak_timepoints_(h)\"] = peak_timepoints\n",
    "            peaks_df[name + \"_peak_heights_(a.u.)\"] = peak_heights\n",
    "        peaks_df.to_csv(f\"..\\\\data\\\\{experiment}_peaks_and_timepoints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_timepoint_csv(experiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_stats_single_experiment(experiment_list:list, experiment_keys:dict) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to perform normalization of the peak data to make them comparable between experiments.\n",
    "    The ratio of the height and the phase shift of the first peak compared to the WT are calculated.\n",
    "    The results are stored in a DataFrame and saved to a csv file.\n",
    "    \"\"\"\n",
    "    for experiment in experiment_list:\n",
    "        # reading in the data for peak height and timepoints\n",
    "        peaks_df = pd.read_csv(f\"..\\\\data\\\\{experiment}_peaks_and_timepoints.csv\", index_col=0)\n",
    "        unique_keys = experiment_keys[experiment]\n",
    "\n",
    "        # DataFrame to store the calculated values in\n",
    "        mean_peaks = pd.DataFrame()\n",
    "\n",
    "        for key in unique_keys:\n",
    "            # sorting for the individual strains\n",
    "            sub_data = peaks_df[[i for i in peaks_df.keys() if re.search(key + r\"_\", i)]]\n",
    "\n",
    "            # only the timepoints\n",
    "            timepoints = sub_data[[i for i in sub_data.keys() if re.search(r\"timepoints\", i)]]\n",
    "\n",
    "            # number of replicates\n",
    "            n = len(timepoints.iloc[0,:])\n",
    "\n",
    "            # calculating the mean timepoint for only the first peak\n",
    "            mean_tp = np.mean(timepoints.iloc[0,:])\n",
    "            # calculating the standard deviation of the timepoint of the first peak\n",
    "            sd_tp = np.std(timepoints.iloc[0,:])\n",
    "\n",
    "            # only the heights of the peaks\n",
    "            heights = sub_data[[i for i in sub_data.keys() if re.search(r\"heights\", i)]]\n",
    "\n",
    "            # calculating the mean height for only the first peak\n",
    "            mean_h = np.mean(heights.iloc[0,:])\n",
    "            # calculating the standard deviation of the height of the first peak\n",
    "            sd_h = np.std(heights.iloc[0,:])\n",
    "\n",
    "            # storing the values of the WT in variables for comparison\n",
    "            if re.search(r\"6803_WT\", key):\n",
    "                WT_mean_tp = mean_tp\n",
    "                WT_sd_tp = sd_tp\n",
    "                WT_mean_h = mean_h\n",
    "                WT_sd_h = sd_h\n",
    "            \n",
    "            # height ratio of each strain compared to the WT\n",
    "            h_ratio = mean_h / WT_mean_h\n",
    "            # error for the height ratio\n",
    "            ratio_error = sd_h / mean_h  + WT_sd_h / WT_mean_h\n",
    "            # difference in timpoint of the first peak of each strain compared to the WT (phase shift)\n",
    "            tp_difference = mean_tp - WT_mean_tp\n",
    "            # error of the phase shift\n",
    "            difference_error = sd_tp + WT_sd_tp\n",
    "\n",
    "            value_list = [mean_tp, sd_tp, mean_h, sd_h, tp_difference, difference_error, h_ratio, ratio_error]\n",
    "\n",
    "            # adding each value to a DataFrame\n",
    "            mean_peaks[key + f\"_n={n}\"] = value_list\n",
    "\n",
    "        # renaming the rows of the DataFrame\n",
    "        mean_peaks = mean_peaks.rename(index={0: \"mean_timepoint\",\n",
    "                        1: \"sd_timepoint\",\n",
    "                        2: \"mean_height\",\n",
    "                        3: \"sd_height\",\n",
    "                        4: \"difference_timepoint_to_WT\",\n",
    "                        5: \"difference_error\",\n",
    "                        6: \"ratio_height_to_WT\",\n",
    "                        7: \"ratio_error\"})\n",
    "                        \n",
    "        # saving to csv\n",
    "        mean_peaks.to_csv(f\"..\\\\data\\\\{experiment}_peak_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_stats_single_experiment(experiment_list, experiment_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot_4x2(color_dic:dict, dimensions:tuple, lw:int) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for plotting the data from the first experiment and the normalized data from all three experiments to a\n",
    "    4x2 subplot.\n",
    "    \"\"\"\n",
    "    ##############################################################################################\n",
    "    def plot_growth_with_sd(data:pd.DataFrame, ax:plt.axis, search_key:str) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for plotting the raw signal from the BioLector with standard deviation and a regression curve \n",
    "        for reference. Which axis to plot to and which strain to plot can be specified with the ax and search_key argument.\n",
    "        \"\"\"\n",
    "\n",
    "        sub_data = data[[i for i in data.keys() if re.search(search_key, i)]]\n",
    "\n",
    "        x = data[\"Time(h)\"]\n",
    "        y1 = sub_data[f\"{search_key}_mean\"]\n",
    "        sd1 = sub_data[f\"{search_key}_sd\"]\n",
    "        y2 = sub_data[f\"{search_key}_reg\"]\n",
    "\n",
    "        for key in color_dic:\n",
    "            if re.search(key, search_key):\n",
    "                color = color_dic[key]\n",
    "\n",
    "        name = search_key.removeprefix(\"6803_\").removesuffix(\"_mean\").replace(\"delta\", \"\\u0394\").replace(\"_\", \" \").replace(\"K\", \"k\")\n",
    "\n",
    "        n = data[[i for i in data.keys() if re.search(search_key + r\"_rep\\d\", i)]].shape[1]\n",
    "\n",
    "        ax.plot(x, y1, color=color, lw=lw, label=f\"{name} \\u00B1 sd; n = {n}\")\n",
    "        ax.fill_between(x, y1+sd1, y1-sd1, color=color, alpha=0.4)\n",
    "        ax.plot(x, y2, color=\"k\", alpha=1, lw=1, ls=\"--\", label=\"Polynomial Regression\")\n",
    "\n",
    "        ax.set_xlim(0, max(x))\n",
    "\n",
    "        ax.set_ylim(150, 400)\n",
    "\n",
    "        \n",
    "\n",
    "        ax.set_xticks(xticks_major)\n",
    "        ax.set_xticklabels(xticks_major)\n",
    "        ax.set_xticks(xticks_minor, minor=True)\n",
    "        ax.set_xticklabels(xticks_minor, minor=True)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=12, length=5)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=9, length=3)\n",
    "\n",
    "\n",
    "        ax.set_ylabel(\"Backscatter [a.u.]\", fontsize=12)\n",
    "\n",
    "        ax.set_xlabel(\"Time [h]\", fontsize=12)\n",
    "    #########################################################################################\n",
    "\n",
    "    #########################################################################################    \n",
    "    def plot_signal_with_sd(data:pd.DataFrame, ax:plt.axis, search_key:str, experiment:str) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for plotting the isolated and smoothed oscillation signal with standard deviation\n",
    "        agains the time. Which axis to plot to and which strains to plot can be specified with the ax and search_key argument.\n",
    "        \"\"\"\n",
    "        sub_data = data[[i for i in data.keys() if re.search(search_key, i)\n",
    "                        and (re.search(r\"mean\", i)\n",
    "                        or re.search(r\"sd\", i))]]\n",
    "\n",
    "\n",
    "        x = data[\"Time(h)\"]\n",
    "\n",
    "        ax.set_xlim(0, max(x))\n",
    "        ax.set_ylim(-7, 7)\n",
    "        ax.set_xticks(xticks_major)\n",
    "        ax.set_xticklabels(xticks_major)\n",
    "        ax.set_xticks(xticks_minor, minor=True)\n",
    "        ax.set_xticklabels(xticks_minor, minor=True)\n",
    "        ax.set_yticks([-6, -4, -2, 0, 2, 4, 6], minor=False)\n",
    "        ax.set_yticklabels([-6, -4, -2, 0, 2, 4, 6], minor=False)\n",
    "        ax.set_yticks([-7, -5, -3, -1, 1, 3, 5, 7], minor=True)\n",
    "\n",
    "\n",
    "        for key in sorted([j.removesuffix(\"_mean\") for j in sub_data.keys() if j.endswith(\"_mean\")]):\n",
    "            y = sub_data[key + \"_mean\"]\n",
    "            sd = sub_data[key + \"_sd\"]\n",
    "\n",
    "            for key2 in color_dic:\n",
    "                if re.search(key2, key):\n",
    "                    color = color_dic[key2]\n",
    "        \n",
    "            n = len(experiment_keys[experiment][key])\n",
    "            \n",
    "            name = key.removeprefix(\"6803_\").removesuffix(\"_smoothed\").replace(\"delta\", \"\\u0394\").replace(\"d\", \"\\u0394\").replace(\"K\", \"k\")\n",
    "            ax.fill_between(x, y-sd, y+sd, color=color, alpha=0.4)\n",
    "            ax.plot(x, y,color=color, lw=lw, label=f\"{name} \\u00B1 sd\\nn = {n}\")\n",
    "\n",
    "        \n",
    "        ax.tick_params(axis=\"both\", length=5, labelsize=12)\n",
    "        ax.tick_params(axis=\"x\", length=3, which=\"minor\", labelsize=9)\n",
    "\n",
    "        ax.set_ylabel(\"Normalized\\nBackscatter [a.u.]\", fontsize=12)\n",
    "        ax.set_xlabel(\"Time [h]\", fontsize=12)\n",
    "    #########################################################################################\n",
    "        \n",
    "    # data of the first experiment with the mean and std beeing taken after the regression analysis\n",
    "    E007_data_avg_last = pd.read_csv(\"..\\\\data\\\\E007_total_data_mean_after.csv\", index_col=0)\n",
    "    # data of the first experiment with mean and std beeing taken befor the regression analysis\n",
    "    E007_data_avg_first = pd.read_csv(\"..\\\\data\\\\E007_total_data_mean_first.csv\", index_col=0)\n",
    "\n",
    "    # data of the 1st peak phase shift and relative amplitude of all three experiments\n",
    "    E007 = pd.read_csv(\"..\\\\data\\\\E007_peak_stats.csv\", index_col=0)\n",
    "    E008 = pd.read_csv(\"..\\\\data\\\\E008_peak_stats.csv\", index_col=0)\n",
    "    E011 = pd.read_csv(\"..\\\\data\\\\E011_peak_stats.csv\", index_col=0)\n",
    "\n",
    "    # list with DataFrames to iterate over\n",
    "    data_list = [E007, E008, E011]\n",
    "\n",
    "    # list of the search keys to use for the plotting function\n",
    "    search_keys = [\"6803_WT\", \"6803_deltaKaiA1B1C1\", \"6803_deltaKaiA3B3C3\", r\"A1B1C1|A3B3C3|WT\",  r\"C3_\", r\"A3B3_|A3_|KaiC3_|B3_\"]\n",
    "\n",
    "    fig = plt.figure(figsize=dimensions, layout=\"constrained\") # , layout=\"constrained\"\n",
    "\n",
    "\n",
    "    gs1 = GridSpec(4, 2, figure=fig)\n",
    "    gs2 = gs1[2, 1].subgridspec(1, 7)\n",
    "    ax1 = fig.add_subplot(gs1[0,0])\n",
    "    ax2 = fig.add_subplot(gs1[1,0])\n",
    "    ax3 = fig.add_subplot(gs1[2,0])\n",
    "    ax4 = fig.add_subplot(gs1[3,0])\n",
    "    ax5 = fig.add_subplot(gs1[0,1])\n",
    "    ax6 = fig.add_subplot(gs1[1,1])  \n",
    "\n",
    "    ax7 = fig.add_subplot(gs2[0, 0:5]) \n",
    "    ax8 = fig.add_subplot(gs2[0,5:7])\n",
    "\n",
    "    ax9 = fig.add_subplot(gs1[3,1])\n",
    "\n",
    "    xticks_major = [i for i in range(0, 85, 24)]\n",
    "    xticks_minor = [i for i in range(0, 85, 12)]\n",
    "\n",
    "    # using the raw signal plotting function on the first three axes\n",
    "    plot_growth_with_sd(E007_data_avg_first, ax1, search_keys[0])\n",
    "    plot_growth_with_sd(E007_data_avg_first, ax2, search_keys[1])\n",
    "    plot_growth_with_sd(E007_data_avg_first, ax3, search_keys[2])\n",
    "\n",
    "    # using the oscillation signal plotting function on axes 4 - 6\n",
    "    plot_signal_with_sd(E007_data_avg_last, ax4, search_keys[3], \"E007\")\n",
    "    plot_signal_with_sd(E007_data_avg_last, ax5, search_keys[4], \"E007\")\n",
    "    plot_signal_with_sd(E007_data_avg_last, ax6, search_keys[5], \"E007\")\n",
    "\n",
    "\n",
    "    ######################################################################################################\n",
    "    # plot of the phase shift and relative amplitude of the first peak of all three experiments with \n",
    "    # discontinous x-axis:\n",
    "\n",
    "    marker_list = [\"o\", \"^\", \"s\"]\n",
    "    exp_list = [\"E007\", \"E008\", \"E011\"]\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        ordered_data = data_list[i]\n",
    "        marker = marker_list[i]\n",
    "        exp = exp_list[i]\n",
    "\n",
    "        marker_scale = []\n",
    "        for key in ordered_data.keys():\n",
    "            n = 1.5 * int(key.split(\"=\")[1])\n",
    "            marker_scale.append(n)\n",
    "\n",
    "        x = ordered_data.loc[\"ratio_height_to_WT\"]\n",
    "        xerr = np.array(ordered_data.loc[\"ratio_error\"] * x)\n",
    "\n",
    "        y = ordered_data.loc[\"difference_timepoint_to_WT\"]\n",
    "        yerr = ordered_data.loc[\"difference_error\"]\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            \n",
    "            for key2 in color_dic:\n",
    "                if re.search(key2, ordered_data.keys()[i]):\n",
    "                    color = color_dic[key2]\n",
    "\n",
    "            label = f\"{exp} \" + ordered_data.keys()[i].removeprefix(\"6803_\").replace(\"_\", \"; \").replace(\"delta\", \"\\u0394\").replace(\"K\", \"k\")\n",
    "            plot = ax7.errorbar(x[i], y[i], xerr=xerr[i], yerr=yerr[i], fmt=marker, ls=\"\", markersize=marker_scale[i], color=color,\n",
    "                        label=label, capsize=3, mec=\"k\", mew=0.5)\n",
    "            [bar.set_alpha(0.6) for bar in plot[2]]\n",
    "            [bar.set_alpha(0.6) for bar in plot[1]]\n",
    "            plot = ax8.errorbar(x[i], y[i], xerr=xerr[i], yerr=yerr[i], fmt=marker, ls=\"\", markersize=marker_scale[i], color=color,\n",
    "                label=label, capsize=3, mec=\"k\", mew=0.5)\n",
    "            [bar.set_alpha(0.6) for bar in plot[2]]\n",
    "            [bar.set_alpha(0.6) for bar in plot[1]]\n",
    "\n",
    "        ax7.set_ylabel(\"Phase Shift\\n$1^{st}$ Peak [h]\", fontsize=12)\n",
    "        ax7.set_ylim([-10, 8])\n",
    "        ax7.set_yticks([-10, -8, -6, -4, -2, 0, 2, 4, 6, 8], minor=False)\n",
    "        ax7.set_yticklabels([-10, -8, -6, -4, -2, 0, 2, 4, 6, 8], minor=False)\n",
    "        ax7.set_yticks([-9, -7, -5, -3, -1, 1, 3, 5, 7], minor=True)\n",
    "\n",
    "\n",
    "        ax7.set_xlabel(\"Relative Amplitude $1^{st}$ Peak [a.u.] \", fontsize=12, x=0.795)\n",
    "        ax7.set_xlim([0, .5])\n",
    "        ax8.set_xlim([0.9, 1.1])\n",
    "        ax7.set_xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5], minor=False)\n",
    "        ax7.set_xticklabels([0, 0.1, 0.2, 0.3, 0.4, 0.5], minor=False)\n",
    "        ax7.set_xticks([0.05, 0.15, 0.25, 0.35, 0.45], minor=True)\n",
    "        ax8.set_xticks([0.9, 1.0, 1.1], minor=False)\n",
    "        ax8.set_xticks([0.95, 1.05], minor=True)\n",
    "        ax8.set_yticklabels([])\n",
    "\n",
    "        ax7.tick_params(axis=\"both\", length=5, labelsize=12)\n",
    "        ax7.tick_params(axis=\"both\", length=3, labelsize=9, which=\"minor\")\n",
    "        ax8.tick_params(axis=\"x\", length=5, labelsize=12)\n",
    "        ax8.tick_params(axis=\"x\", length=3, labelsize=9, which=\"minor\")\n",
    "\n",
    "        ax7.spines['right'].set_visible(False)\n",
    "        ax8.spines['left'].set_visible(False)\n",
    "        ax8.tick_params(axis=\"y\", which=\"both\", left=False)\n",
    "\n",
    "    ##############################################################################################################\n",
    "\n",
    "\n",
    "    # plot legend to last subplot\n",
    "\n",
    "    custom_lines1 = [Line2D([0], [0], color=\"forestgreen\", lw=4),\n",
    "                    Line2D([0], [0], color=\"orchid\", lw=4),\n",
    "                    Line2D([0], [0], color=\"dodgerblue\", lw=4),\n",
    "                    Line2D([0], [0], color=\"darkorange\", lw=4),\n",
    "                    Line2D([0], [0], color=\"lawngreen\", lw=4),\n",
    "                    Line2D([0], [0], color=\"mediumturquoise\", lw=4),\n",
    "                    Line2D([0], [0], color=\"brown\", lw=4),\n",
    "                    Line2D([0], [0], color=\"olive\", lw=4),\n",
    "                    Line2D([0], [0], color=\"crimson\", lw=4),\n",
    "                    Line2D([0], [0], color=\"k\", lw=1, ls=\"--\", alpha=1)]\n",
    "\n",
    "    custom_labels1 = [\"WT\", \"$\\Delta$kaiA1B1C1\", \"$\\Delta$kaiA3B3C3\", \"$\\Delta$kaiC3\", \"$\\Delta$kaiA3C3\", \"$\\Delta$kaiB3C3\", \"$\\Delta$kaiA3\",\n",
    "                    \"$\\Delta$kaiA3B3\", \"$\\Delta$kaiB3\", \"Polynomial\\nRegression\"]\n",
    "\n",
    "    custom_lines2 = [Line2D([0], [0], marker=\"o\", color=\"grey\", ms=10, ls=\"\"),\n",
    "                    Line2D([0], [0], marker=\"^\", color=\"grey\", ms=10, ls=\"\"),\n",
    "                    Line2D([0], [0], marker=\"s\", color=\"grey\", ms=10, ls=\"\")]\n",
    "\n",
    "    custom_labels2 = [\"Experiment 1\", \"Experiment 2\", \"Experiment 3\"]\n",
    "\n",
    "    ax9.legend(frameon=False, handles=custom_lines1, labels=custom_labels1, loc=9, fontsize=12, ncol=2)\n",
    "\n",
    "    ax10 = ax9.twinx()\n",
    "    ax10.legend(frameon=False, handles=custom_lines2, labels=custom_labels2, loc=8, fontsize=12, ncol=1, bbox_to_anchor=((0.5, -0.15)))\n",
    "\n",
    "    ax9.set_axis_off()\n",
    "    ax10.set_axis_off()\n",
    "\n",
    "    \"\"\"fig.text(0, 0.975, \"A\", fontsize=20)\n",
    "    fig.text(0, 0.975/4*3, \"B\", fontsize=20)\n",
    "    fig.text(0, 0.975/4*2, \"C\", fontsize=20)\n",
    "    fig.text(0, 0.975/4*1, \"D\", fontsize=20)\n",
    "    fig.text(0.5, 0.975, \"E\", fontsize=20)\n",
    "    fig.text(0.5, 0.975/4*3, \"F\", fontsize=20)\n",
    "    fig.text(0.5, 0.975/4*2, \"G\", fontsize=20)\"\"\"\n",
    "\n",
    "    fig.text(0, 0.975, \"a\", fontsize=20)\n",
    "    fig.text(0, 0.725, \"b\", fontsize=20)\n",
    "    fig.text(0, 0.475, \"c\", fontsize=20)\n",
    "    fig.text(0, 0.225, \"d\", fontsize=20)\n",
    "    fig.text(0.5, 0.975, \"e\", fontsize=20)\n",
    "    fig.text(0.5, 0.725, \"f\", fontsize=20)\n",
    "    fig.text(0.5, 0.475, \"g\", fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(\"..\\\\plots\\\\multiplot_4x2.png\", dpi=500)\n",
    "    fig.savefig(\"..\\\\plots\\\\multiplot_4x2.tif\", dpi=500)\n",
    "    fig.savefig(\"..\\\\plots\\\\multiplot_4x2.pdf\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot_4x2(color_dic=color_dic, dimensions=dimensions1, lw=lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot_3x2(experiment:str, color_dic:dict, dimensions:tuple, lw:int) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for the creation of a 2 x 3 plot with the isolated oscillation signals of experiments 2 and 3.\n",
    "    \"\"\"\n",
    "        \n",
    "    #########################################################################################    \n",
    "    def plot_signal_with_sd(data:pd.DataFrame, ax:plt.axis, search_key:str, experiment:str) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for plotting the isolated and smoothed oscillation signal with standard deviation\n",
    "        agains the time. Which axis to plot to and which strains to plot can be specified with the ax and search_key argument.\n",
    "        \"\"\"\n",
    "        sub_data = data[[i for i in data.keys() if re.search(search_key, i)\n",
    "                        and (re.search(r\"mean\", i)\n",
    "                        or re.search(r\"sd\", i))]]\n",
    "\n",
    "\n",
    "        x = data[\"Time(h)\"]\n",
    "\n",
    "        ax.set_xlim(0, max(x))\n",
    "        ax.set_ylim(-7, 7)\n",
    "        ax.set_xticks(xticks_major)\n",
    "        ax.set_xticklabels(xticks_major)\n",
    "        ax.set_xticks(xticks_minor, minor=True)\n",
    "        ax.set_xticklabels(xticks_minor, minor=True)\n",
    "        ax.set_yticks([-6, -4, -2, 0, 2, 4, 6], minor=False)\n",
    "        ax.set_yticklabels([-6, -4, -2, 0, 2, 4, 6], minor=False)\n",
    "        ax.set_yticks([-7, -5, -3, -1, 1, 3, 5, 7], minor=True)\n",
    "\n",
    "\n",
    "        for key in sorted([j.removesuffix(\"_mean\") for j in sub_data.keys() if j.endswith(\"_mean\")]):\n",
    "            y = sub_data[key + \"_mean\"]\n",
    "            sd = sub_data[key + \"_sd\"]\n",
    "\n",
    "            for key2 in color_dic:\n",
    "                if re.search(key2, key):\n",
    "                    color = color_dic[key2]\n",
    "        \n",
    "            n = len(experiment_keys[experiment][key])\n",
    "            \n",
    "            name = key.removeprefix(\"6803_\").removesuffix(\"_smoothed\").replace(\"delta\", \"\\u0394\").replace(\"d\", \"\\u0394\").replace(\"K\", \"k\")\n",
    "            ax.fill_between(x, y-sd, y+sd, color=color, alpha=0.4)\n",
    "            ax.plot(x, y,color=color, lw=lw, label=f\"{name} \\u00B1 sd; n = {n}\")\n",
    "\n",
    "        \n",
    "        ax.tick_params(axis=\"both\", length=5, labelsize=12)\n",
    "        ax.tick_params(axis=\"x\", length=3, which=\"minor\", labelsize=9)\n",
    "\n",
    "        #ax.set_ylabel(\"Normalized\\nBackscatter [a.u.]\", fontsize=12)\n",
    "        #ax.set_xlabel(\"Time [h]\", fontsize=12)\n",
    "        ax.legend(frameon=False, fontsize=9, ncol=1, loc=4)\n",
    "    #########################################################################################\n",
    "        \n",
    "    # data of the first experiment with the mean and std beeing taken after the regression analysis\n",
    "    E008_avg_last = pd.read_csv(f\"..\\\\data\\\\E008_total_data_mean_after.csv\", index_col=0)\n",
    "    # data of the first experiment with mean and std beeing taken befor the regression analysis\n",
    "    E011_avg_last = pd.read_csv(f\"..\\\\data\\\\E011_total_data_mean_after.csv\", index_col=0)\n",
    "\n",
    "    if experiment == \"E011\":\n",
    "        search_keys = [r\"A1B1C1|WT\",  r\"C3_\", r\"A3B3_|A3_|KaiC3_|B3_\"]\n",
    "    else:\n",
    "        search_keys = [r\"A1B1C1|A3B3C3|WT\",  r\"C3_\", r\"A3B3_|A3_|KaiC3_|B3_\"]\n",
    "\n",
    "    fig, ((ax1, ax2, ax3),\n",
    "          (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=dimensions, layout=\"constrained\", sharex=True, sharey=True)#\n",
    "    \n",
    "    xticks_major = [i for i in range(0, 85, 24)]\n",
    "    xticks_minor = [i for i in range(0, 85, 12)]\n",
    "\n",
    "\n",
    "    # using the oscillation signal plotting function on axes 1 - 6\n",
    "    plot_signal_with_sd(E008_avg_last, ax1, search_keys[0], experiment)\n",
    "    plot_signal_with_sd(E008_avg_last, ax2, search_keys[1], experiment)\n",
    "    plot_signal_with_sd(E008_avg_last, ax3, search_keys[2], experiment)\n",
    "    plot_signal_with_sd(E011_avg_last, ax4, search_keys[0], experiment)\n",
    "    plot_signal_with_sd(E011_avg_last, ax5, search_keys[1], experiment)\n",
    "    plot_signal_with_sd(E011_avg_last, ax6, search_keys[2], experiment)\n",
    "\n",
    "    ax1.set_ylabel(\"Normalized\\nBackscatter [a.u.]\", fontsize=12)\n",
    "    ax4.set_ylabel(\"Normalized\\nBackscatter [a.u.]\", fontsize=12)\n",
    "    ax2.set_ylabel(\" \", fontsize=12)\n",
    "    ax3.set_ylabel(\" \", fontsize=12)\n",
    "    ax5.set_ylabel(\" \", fontsize=12)\n",
    "    ax6.set_ylabel(\" \", fontsize=12)\n",
    "\n",
    "\n",
    "    ax1.set_xlabel(\" \", fontsize=12)\n",
    "    ax2.set_xlabel(\" \", fontsize=12)\n",
    "    ax3.set_xlabel(\" \", fontsize=12)\n",
    "    ax4.set_xlabel(\"Time [h]\", fontsize=12)\n",
    "    ax5.set_xlabel(\"Time [h]\", fontsize=12)\n",
    "    ax6.set_xlabel(\"Time [h]\", fontsize=12)\n",
    "\n",
    "    ax1.set_title(\" \", fontsize=12)\n",
    "    ax2.set_title(\" \", fontsize=12)\n",
    "    ax3.set_title(\" \", fontsize=12)\n",
    "\n",
    "    fig.text(0.05, 0.999, \"a\", fontsize=20, ha=\"left\", va=\"top\")\n",
    "    fig.text(0.365, 0.999, \"b\", fontsize=20, ha=\"left\", va=\"top\")\n",
    "    fig.text(0.68, 0.999, \"c\", fontsize=20, ha=\"left\", va=\"top\")\n",
    "    fig.text(0.05, 0.485, \"d\", fontsize=20, ha=\"left\", va=\"bottom\")\n",
    "    fig.text(0.365, 0.485, \"e\", fontsize=20, ha=\"left\", va=\"bottom\")\n",
    "    fig.text(0.68, 0.485, \"f\", fontsize=20, ha=\"left\", va=\"bottom\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(f\"..\\\\plots\\\\supplement_multiplot_2x3.png\", dpi=500)\n",
    "    fig.savefig(f\"..\\\\plots\\\\supplement_multiplot_2x3.tif\", dpi=500)\n",
    "    fig.savefig(f\"..\\\\plots\\\\supplement_multiplot_2x3.pdf\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot_3x2(\"E008\", color_dic, dimensions2, lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
