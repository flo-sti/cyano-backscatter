{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import levene\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the experiments for naming of the files\n",
    "experiment_list = [\"E007\", \"E008\", \"E011\"]\n",
    "\n",
    "# list of the names of the excel files with the raw data of the experiments\n",
    "file_name_list = [\"..\\\\data\\\\\" + i for i in os.listdir(\"..\\\\data\") if i.endswith(\".xlsx\")]\n",
    "file_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion factor from inches to cm for the size of the plots\n",
    "cm_in_inches = 2.54\n",
    "width = 21 * 2\n",
    "height = 27.6 * 2\n",
    "dimensions1 = (height/cm_in_inches, width/cm_in_inches)\n",
    "dimensions2 = (width/cm_in_inches, height/2/cm_in_inches)\n",
    "lw = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with the colors associated with the strains contained in the experiments\n",
    "color_dic = {r\"WT\": \"forestgreen\",\n",
    "             r\"KaiA3\": \"brown\",\n",
    "             r\"KaiB3\": \"crimson\",\n",
    "             r\"KaiC3\": \"darkorange\",\n",
    "             r\"KaiA3B3\": \"olive\",\n",
    "             r\"KaiA3C3\": \"lawngreen\",\n",
    "             r\"KaiB3C3\": \"mediumturquoise\",\n",
    "             r\"KaiA3B3C3\": \"dodgerblue\",\n",
    "             r\"KaiA1B1C1\": \"orchid\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renaming_of_raw_data(file_name_list:list, experiments:list, save=False) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    This function reads in the raw data as a pandas DataFrame and renames the\n",
    "    columns based on the names given to each well in the BioLection software.\n",
    "    The data is then trimmed to the first 84 h and unnecessary strains are removed\n",
    "    and the result is saved to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    for file, experiment in zip(file_name_list, experiments):\n",
    "        # raw data is saved in sheet 4 of the excel file\n",
    "        raw_data = pd.read_excel(file, sheet_name=3, skiprows=range(1,4))\n",
    "        keys = raw_data.keys()[2:]\n",
    "        keys_dict = {i: re.sub(r\"Ch\\d_\", r\"\", j) for i, j in zip(keys, keys)}\n",
    "        raw_data = raw_data.rename(columns=keys_dict)\n",
    "        raw_data = raw_data[[i for i in raw_data.keys() if i.startswith(\"raw\") or i.startswith(\"Time[h]\")]]\n",
    "\n",
    "\n",
    "        my_names = [i.replace(\"raw_\", \"\") for i in raw_data.keys()]\n",
    "        # the names are saved in sheet 2 of the excel file\n",
    "        names = pd.read_excel(file, sheet_name=1, skiprows=range(1,4))\n",
    "        names = names.query(\"Well == @my_names\")\n",
    "        names_dict = {\"raw_\" + i: j.replace(\",\", \"\").replace(\" \", \"_\").replace(\"Uppsala_\", \"\") for i, j in zip(names[\"Well\"], names[\"Description\"])}\n",
    "        raw_data = raw_data.rename(columns=names_dict)\n",
    "\n",
    "        # trimming the data to the first 84 h\n",
    "        index_end = raw_data[raw_data[\"Time[h]\"] <= 84].shape[0] + 1 \n",
    "        raw_data = raw_data.iloc[:index_end,]   \n",
    "\n",
    "        # trimming strains that are not needed:\n",
    "        raw_data = raw_data[[key for key in raw_data.keys() if not re.search(r\"7942|mVenus|glgC\", key)]] #|Wilde\n",
    "\n",
    "        # saving the result to a csv file:\n",
    "        if save:\n",
    "            raw_data.to_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renaming_of_raw_data(file_name_list, experiment_list, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_unique_keys(experiment_list:list) -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for identifying the unique part of the names of the DataFrame\n",
    "    and saving them in a dictionary sorted by the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    experiments_2_key_2_names = {}\n",
    "    for experiment in experiment_list:\n",
    "        data = pd.read_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\", index_col=0)\n",
    "        unique_keys = {}\n",
    "        for i in data.keys()[1:]:\n",
    "            unique_part = \"_\".join(i.split(\"_\")[0:2])\n",
    "            if unique_part not in unique_keys:\n",
    "                unique_keys[unique_part] = []\n",
    "            unique_keys[unique_part].append(i)\n",
    "\n",
    "        experiments_2_key_2_names[experiment] = unique_keys\n",
    "    return experiments_2_key_2_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_keys = identify_unique_keys(experiment_list=experiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(y:pd.Series, kernel:np.ndarray, poly=4) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Take a pd.Series from the raw data and fit a polynomial regression of the 4th degree to it.\n",
    "    The smoothed signal, signal and polynomial regression are returned. Function adapted from\n",
    "    Berwanger et al. 2023 (https://doi.org/10.1101/2023.09.26.559469).\n",
    "    \"\"\"\n",
    "    # 4th degree polynomial regression\n",
    "    y_reg = np.array(np.arange(len(y))).reshape(-1,1)\n",
    "    pf = PolynomialFeatures(poly)\n",
    "    y_reg = pf.fit_transform(y_reg)\n",
    "    reg_fit = LinearRegression().fit(y_reg, y)\n",
    "    reg_predict = reg_fit.predict(y_reg)\n",
    "\n",
    "    # the polynomial regression is subtracted from the raw data\n",
    "    signal = y - reg_predict\n",
    "    # from the result, the mean is subtracted\n",
    "    signal -= signal.mean()\n",
    "    #the signal is smoothened by a rolling average with the kernel size given to the function\n",
    "    smoothed_signal = np.convolve(signal, kernel, mode='same')\n",
    "    return smoothed_signal, signal, reg_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_first_data(experiment_list:list, experiment_keys:dict) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for calculating the average of all replicates of one experiment\n",
    "    and performing the regression analysis on it afterwards.\n",
    "    The results are saved to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    for experiment in experiment_list:\n",
    "\n",
    "        # reading in the trimmed and renamed data\n",
    "        raw_data = pd.read_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\", index_col=0)\n",
    "        unique_keys = experiment_keys[experiment]\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        data[\"Time(h)\"] = raw_data[\"Time[h]\"]\n",
    "\n",
    "        kernel_size = 40\n",
    "        kernel = np.ones(kernel_size) / kernel_size\n",
    "\n",
    "        # the first three hours are cut off to eliminate measuring artifacts\n",
    "        index_1 = data[\"Time(h)\"][data[\"Time(h)\"] >= 3].index[0]\n",
    "\n",
    "        for key in unique_keys:\n",
    "            keys = unique_keys[key]\n",
    "            sub_data = raw_data[keys]\n",
    "\n",
    "            for i in sub_data.keys():\n",
    "                data[i] = sub_data[i]\n",
    "\n",
    "            mean = np.mean(sub_data, axis=1)\n",
    "            std = np.std(sub_data, axis=1)\n",
    "\n",
    "            mean_name = key + \"_mean\"\n",
    "            sd_name = key + \"_sd\"\n",
    "            reg_name = key + \"_reg\"\n",
    "            sig_name = key + \"_signal\"\n",
    "            smooth_name = key + \"_smoothed\"\n",
    "\n",
    "            data[mean_name] = mean\n",
    "            data[sd_name] = std\n",
    "\n",
    "            # regression analysis\n",
    "            smoothed_signal, signal, reg_predict = polynomial_regression(data[mean_name][index_1:], kernel)\n",
    "\n",
    "            smoothed_signal[smoothed_signal.shape[0] - kernel_size:] = np.nan\n",
    "            smoothed_signal[:kernel_size] = np.nan\n",
    "\n",
    "            nan_arr = np.array([np.nan for i in range(0, data.shape[0] - smoothed_signal.shape[0])])\n",
    "            smoothed_signal = pd.DataFrame(np.concatenate([nan_arr, smoothed_signal], axis=0), columns=[smooth_name])\n",
    "            signal = pd.DataFrame(np.concatenate([nan_arr, signal], axis=0), columns=[sig_name])\n",
    "            reg_predict = pd.DataFrame(np.concatenate([nan_arr, reg_predict], axis=0), columns=[reg_name])\n",
    "\n",
    "            data = pd.concat([data, reg_predict, signal, smoothed_signal], axis=1, ignore_index=False)\n",
    "        data.to_csv(f\"..\\\\data\\\\{experiment}_total_data_mean_first.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_first_data(experiment_list, experiment_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_last_data(experiment_list:list, experiment_keys_keys:dict) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for performing the regression analysis for each replicate first\n",
    "    and then taking the average of the resulting smoothed signal.\n",
    "    The results are saved to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    for experiment in experiment_list:\n",
    "        \n",
    "        # reading in the trimmed data set\n",
    "        raw_data = pd.read_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\", index_col=0)\n",
    "        unique_keys = experiment_keys[experiment]\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        data[\"Time(h)\"] = raw_data[\"Time[h]\"]\n",
    "\n",
    "        kernel_size = 40\n",
    "        kernel = np.ones(kernel_size) / kernel_size\n",
    "        \n",
    "        # the first three hours are cut off to eliminate measuring artifacts\n",
    "        index_1 = data[\"Time(h)\"][data[\"Time(h)\"] >= 3].index[0]\n",
    "\n",
    "        cutoff = data.shape[0] - data[index_1:].shape[0]\n",
    "\n",
    "        for key in unique_keys:\n",
    "            keys = unique_keys[key]\n",
    "            sub_data = raw_data[keys]\n",
    "\n",
    "            for i in sub_data.keys():\n",
    "                data[i] = sub_data[i]\n",
    "\n",
    "                reg_name = i + \"_reg\"\n",
    "                sig_name = i + \"_signal\"\n",
    "                smooth_name = i + \"_smoothed\"\n",
    "\n",
    "                smoothed_signal, signal, reg_predict = polynomial_regression(data[i][index_1:], kernel)\n",
    "\n",
    "                smoothed_signal[smoothed_signal.shape[0] - kernel_size:] = np.nan\n",
    "                smoothed_signal[:kernel_size] = np.nan\n",
    "\n",
    "                nan_arr = np.array([np.nan for i in range(0, cutoff)])\n",
    "                smoothed_signal = pd.DataFrame(np.concatenate([nan_arr, smoothed_signal], axis=0), columns=[smooth_name])\n",
    "                signal = pd.DataFrame(np.concatenate([nan_arr, signal], axis=0), columns=[sig_name])\n",
    "                reg_predict = pd.DataFrame(np.concatenate([nan_arr, reg_predict], axis=0), columns=[reg_name])\n",
    "                data = pd.concat([data, reg_predict, signal, smoothed_signal], axis=1, ignore_index=False)\n",
    "\n",
    "            smoothed_data = data[[j for j in data.keys() if re.search(r\"smoothed\", j) and re.search(key + \"_\", j)]][cutoff:]\n",
    "\n",
    "            mean_name = key + \"_mean\"\n",
    "            sd_name = key + \"_sd\"\n",
    "\n",
    "            mean = np.mean(smoothed_data, axis=1)\n",
    "            std = np.std(smoothed_data, axis=1)\n",
    "\n",
    "            mean = pd.DataFrame(np.concatenate([nan_arr, mean], axis=0), columns=[mean_name])\n",
    "            std = pd.DataFrame(np.concatenate([nan_arr, std], axis=0), columns=[sd_name])\n",
    "\n",
    "            data[mean_name] = mean\n",
    "            data[sd_name] = std\n",
    "\n",
    "        data.to_csv(f\"..\\\\data\\\\{experiment}_total_data_mean_after.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_last_data(experiment_list, experiment_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_timepoint_csv(experiment_list:list) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for identifying the peaks in the smoothed signal.\n",
    "    The timepoints and heights of the peaks for each strain are saved to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    for experiment in experiment_list:\n",
    "        data = pd.read_csv(f\"..\\\\data\\\\{experiment}_total_data_mean_after.csv\", index_col=0)\n",
    "        sub_data = data[[key for key in data.keys() if re.search(r\"rep\\d_smoothed|Time\", key)]]\n",
    "        peaks_df = pd.DataFrame()\n",
    "        for key in sub_data.iloc[:,1:]:\n",
    "            mean = np.mean(sub_data[key])\n",
    "            maximum = np.max(sub_data[key])\n",
    "\n",
    "            # identifying of the peaks:\n",
    "            #peaks need to be separated by 150 indices, have a width of at least 65 indices and a height of at least the mean + 20 % of the highest value\n",
    "            peaks = find_peaks(sub_data[key], distance=150, width=65, height=mean+0.2*maximum) # 150 <=> 12.5 h; 65 <=> 5.42 h\n",
    "            peaks = peaks[0]\n",
    "            peak_timepoints = np.array([])\n",
    "            peak_heights = np.array([])\n",
    "            name = \"{}_{}_{}\".format(key.split(\"_\")[0], key.split(\"_\")[1], key.split(\"_\")[2])\n",
    "            for peak in peaks:\n",
    "                # only peaks after 12 h are considered\n",
    "                if sub_data[\"Time(h)\"][peak] >= 12:\n",
    "                    peak_timepoints = np.append(peak_timepoints, np.array(sub_data[\"Time(h)\"][peak]))\n",
    "                    peak_heights = np.append(peak_heights, np.array(sub_data[key][peak]))\n",
    "                else:\n",
    "                    continue\n",
    "            # arrays with less than 3 peak timepoints/heights are filled with np.nan so that the results can be put into a DataFrame            \n",
    "            if peak_timepoints.shape[0] < 3:\n",
    "                for i in range(0, 3 - peak_timepoints.shape[0]):\n",
    "                    peak_timepoints = np.append(peak_timepoints, np.nan)\n",
    "                    peak_heights = np.append(peak_heights, np.nan)\n",
    "            peaks_df[name + \"_peak_timepoints_(h)\"] = peak_timepoints\n",
    "            peaks_df[name + \"_peak_heights_(a.u.)\"] = peak_heights\n",
    "        peaks_df.to_csv(f\"..\\\\data\\\\{experiment}_peaks_and_timepoints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_timepoint_csv(experiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_stats_single_experiment(experiment_list:list, experiment_keys:dict) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to perform normalization of the peak data to make them comparable between experiments.\n",
    "    The ratio of the height and the phase shift of the first peak compared to the WT are calculated.\n",
    "    The results are stored in a DataFrame and saved to a csv file.\n",
    "    \"\"\"\n",
    "    for experiment in experiment_list:\n",
    "        # reading in the data for peak height and timepoints\n",
    "        peaks_df = pd.read_csv(f\"..\\\\data\\\\{experiment}_peaks_and_timepoints.csv\", index_col=0)\n",
    "        unique_keys = experiment_keys[experiment]\n",
    "\n",
    "        # DataFrame to store the calculated values in\n",
    "        mean_peaks = pd.DataFrame()\n",
    "\n",
    "        for key in unique_keys:\n",
    "            # sorting for the individual strains\n",
    "            sub_data = peaks_df[[i for i in peaks_df.keys() if re.search(key + r\"_\", i)]]\n",
    "\n",
    "            # only the timepoints\n",
    "            timepoints = sub_data[[i for i in sub_data.keys() if re.search(r\"timepoints\", i)]]\n",
    "\n",
    "            # number of replicates\n",
    "            n = len(timepoints.iloc[0,:])\n",
    "\n",
    "            # calculating the mean timepoint for only the first peak\n",
    "            mean_tp = np.mean(timepoints.iloc[0,:])\n",
    "            # calculating the standard deviation of the timepoint of the first peak\n",
    "            sd_tp = np.std(timepoints.iloc[0,:])\n",
    "\n",
    "            # only the heights of the peaks\n",
    "            heights = sub_data[[i for i in sub_data.keys() if re.search(r\"heights\", i)]]\n",
    "\n",
    "            # calculating the mean height for only the first peak\n",
    "            mean_h = np.mean(heights.iloc[0,:])\n",
    "            # calculating the standard deviation of the height of the first peak\n",
    "            sd_h = np.std(heights.iloc[0,:])\n",
    "\n",
    "            # storing the values of the WT in variables for comparison\n",
    "            if re.search(r\"6803_WT\", key):\n",
    "                WT_mean_tp = mean_tp\n",
    "                WT_sd_tp = sd_tp\n",
    "                WT_mean_h = mean_h\n",
    "                WT_sd_h = sd_h\n",
    "            \n",
    "            # height ratio of each strain compared to the WT\n",
    "            h_ratio = mean_h / WT_mean_h\n",
    "            # error for the height ratio\n",
    "            ratio_error = sd_h / mean_h  + WT_sd_h / WT_mean_h\n",
    "            # difference in timpoint of the first peak of each strain compared to the WT (phase shift)\n",
    "            tp_difference = mean_tp - WT_mean_tp\n",
    "            # error of the phase shift\n",
    "            difference_error = sd_tp + WT_sd_tp\n",
    "\n",
    "            value_list = [mean_tp, sd_tp, mean_h, sd_h, tp_difference, difference_error, h_ratio, ratio_error]\n",
    "\n",
    "            # adding each value to a DataFrame\n",
    "            mean_peaks[key + f\"_n={n}\"] = value_list\n",
    "\n",
    "        # renaming the rows of the DataFrame\n",
    "        mean_peaks = mean_peaks.rename(index={0: \"mean_timepoint\",\n",
    "                        1: \"sd_timepoint\",\n",
    "                        2: \"mean_height\",\n",
    "                        3: \"sd_height\",\n",
    "                        4: \"difference_timepoint_to_WT\",\n",
    "                        5: \"difference_error\",\n",
    "                        6: \"ratio_height_to_WT\",\n",
    "                        7: \"ratio_error\"})\n",
    "                        \n",
    "        # saving to csv\n",
    "        mean_peaks.to_csv(f\"..\\\\data\\\\{experiment}_peak_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_stats_single_experiment(experiment_list, experiment_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renamed_data_to_separate_csvs(filepath:str, kernel_size:int) -> None:\n",
    "\n",
    "    \"\"\" Function to read out the renamed raw data and perform the regression analysis to save the results into separate csv files\"\"\"\n",
    "\n",
    "    # kernel for the smoothing of the signal\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "    \n",
    "\n",
    "    # data read in\n",
    "    data = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    # determine from the filepath which experiment it is\n",
    "    experiment = re.findall(r\"E\\d\\d\\d\", filepath)[0]\n",
    "\n",
    "    # empty DataFrames for storage of the results\n",
    "    smooth_df = pd.DataFrame()\n",
    "    sig_df = pd.DataFrame()\n",
    "    reg_df = pd.DataFrame()\n",
    "\n",
    "    # only data from 3 h onwards are considered\n",
    "    index_1 = data[\"Time[h]\"][data[\"Time[h]\"] >= 3].index[0]\n",
    "    data = data[index_1:]\n",
    "\n",
    "    # inserting the Time [h] column into each DataFrame,\n",
    "    # for the smoothed data the kernel size is cut of from both ends because the results at the ends are not reliable\n",
    "    smooth_df[data.keys()[0]] = data[data.keys()[0]][kernel_size:len(data[data.keys()[0]])-kernel_size]\n",
    "    sig_df[data.keys()[0]] = data[data.keys()[0]]\n",
    "    reg_df[data.keys()[0]] = data[data.keys()[0]]\n",
    "\n",
    "    # loop through the data (excluding the time),\n",
    "    # performing the regression analysis and \n",
    "    # storing the results in the separate DataFrames\n",
    "    for column in data.keys()[1:]:\n",
    "        smooth, sig, reg = polynomial_regression(data[column], kernel)\n",
    "        smooth_df[column] = smooth[kernel_size:len(smooth)-kernel_size]\n",
    "        sig_df[column] = sig\n",
    "        reg_df[column] = reg\n",
    "\n",
    "    # reindexing of the DataFrames\n",
    "    smooth_df = smooth_df.reindex(index=pd.Index(range(data.shape[0]+index_1)))\n",
    "    sig_df = sig_df.reindex(index=pd.Index(range(data.shape[0]+index_1)))\n",
    "    reg_df = reg_df.reindex(index=pd.Index(range(data.shape[0]+index_1)))\n",
    "\n",
    "    # saving the DataFrames into separate csv files\n",
    "    smooth_df.to_csv(f\"..\\\\data\\\\{experiment}_smoothed_data.csv\")\n",
    "    sig_df.to_csv(f\"..\\\\data\\\\{experiment}_signal_data.csv\")\n",
    "    reg_df.to_csv(f\"..\\\\data\\\\{experiment}_polynomial_regression_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"..\\\\data\\\\E007_renamed_data.csv\",\n",
    "         \"..\\\\data\\\\E008_renamed_data.csv\",\n",
    "         \"..\\\\data\\\\E011_renamed_data.csv\"]\n",
    "for file in files:\n",
    "        renamed_data_to_separate_csvs(filepath=file, kernel_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_first_period(search_key:str, files:list) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" Function to calculate the absolute of the time difference between the first trough and first peak and \n",
    "    multiply by two to determine an estimate for the (initial) period of the (damped) oscillation.\n",
    "    (partial) results are saved as csv files. Which strain is analyzed is determined by the search key.\"\"\"\n",
    "\n",
    "    # extracting the experiment names from the list of file names for naming of the output files\n",
    "    experiments = [re.findall(r\"E\\d\\d\\d\", string)[0] for string in files]\n",
    "\n",
    "    # loading the data from files\n",
    "    smoothed = [pd.read_csv(file, index_col=0) for file in files]\n",
    "\n",
    "    # empty DataFrame for the results\n",
    "    experiment_df = pd.DataFrame()\n",
    "    \n",
    "    # looping through the list of files\n",
    "    for i in range(len(smoothed)):\n",
    "\n",
    "        # condition to check that the search key is present in the experiment data\n",
    "        if len([key for key in smoothed[i].keys() if re.search(search_key, key)]) > 0:\n",
    "\n",
    "            experiment = experiments[i]\n",
    "            experiment_df[f\"{experiment}_Time[h]\"] = smoothed[i][\"Time[h]\"]\n",
    "\n",
    "            # depending on the experiment, some replicates are discarded due to technical artifacts in the data\n",
    "            if experiment == \"E007\":\n",
    "                for key in sorted([key for key in smoothed[i].keys() if re.search(search_key, key)\n",
    "                                   and not re.search(r\"Wilde\", key)]):\n",
    "                    experiment_df[experiment + \"_\" + key] = smoothed[i][key]\n",
    "            elif experiment == \"E008\":\n",
    "                for key in sorted([key for key in smoothed[i].keys() if re.search(search_key, key)\n",
    "                                and not re.search(r\"Wilde\", key)]):\n",
    "                    experiment_df[experiment + \"_\" + key] = smoothed[i][key]\n",
    "            elif experiment == \"E011\":\n",
    "                for key in sorted([key for key in smoothed[i].keys() if re.search(search_key, key)\n",
    "                                and not re.search(r\"Wilde\", key)]):\n",
    "                    experiment_df[experiment + \"_\" + key] = smoothed[i][key]\n",
    "\n",
    "    # data from all experiments are concatenated into one file               \n",
    "    experiment_df.to_csv(f\"..\\\\data\\\\{search_key}_all_experiments_smoothed_data.csv\")\n",
    "\n",
    "    # empty DataFrames for the first peaks and troghs\n",
    "    peaks_df = pd.DataFrame()\n",
    "    troughs_df = pd.DataFrame()\n",
    "\n",
    "    # peaks and troughs of the strain are determined for each experiment in the DataFrame\n",
    "    for exp in experiments:\n",
    "        for key in [key for key in experiment_df.keys() if re.search(exp, key) and not re.search(r\"Time\", key)]:\n",
    "\n",
    "            time = experiment_df[f\"{exp}_Time[h]\"]\n",
    "\n",
    "            # peaks have to be at least 150 measurements (~12.5 h) apart and have a width of 50 measurements (~4.2 h)\n",
    "            peaks = find_peaks(experiment_df[key], distance=150, width=50)\n",
    "            troughs = find_peaks(experiment_df[key]*-1, distance=150, width=50)\n",
    "\n",
    "            # only for the first peak/trough\n",
    "            peaks_t = np.array(time.iloc[peaks[0]])[0]\n",
    "            troughs_t = np.array(time.iloc[troughs[0]])[0]\n",
    "\n",
    "            # first peak/trough are added to the DataFrames\n",
    "            peaks_df.loc[0, key] = peaks_t\n",
    "            troughs_df.loc[0, key] = troughs_t\n",
    "\n",
    "    # DataFrames are saved as csv files\n",
    "    peaks_df.to_csv(f\"..\\\\data\\\\{search_key}_all_experiments_peaks.csv\")\n",
    "    troughs_df.to_csv(f\"..\\\\data\\\\{search_key}_all_experiments_troughs.csv\")\n",
    "\n",
    "\n",
    "    # empty DataFrame for the first period\n",
    "    first_period_df = pd.DataFrame()\n",
    "\n",
    "    # first period is calculated by subtracting the time of the first trough from the time of\n",
    "    # the first peak, taking the absolute of the result and multiplying the result by 2\n",
    "    # the result is added to the DataFrame\n",
    "    for key in peaks_df.keys():\n",
    "        first_period_df[key] = np.abs((peaks_df[key] - troughs_df[key]) * 2)\n",
    "\n",
    "    # the DataFrame is saved to a new csv file\n",
    "    first_period_df.to_csv(f\"..\\\\data\\\\{search_key}_all_experiments_first_period.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths to the data frames\n",
    "smoothed_files = [\"..\\\\data\\\\E007_smoothed_data.csv\",\n",
    "                \"..\\\\data\\\\E008_smoothed_data.csv\",\n",
    "                \"..\\\\data\\\\E011_smoothed_data.csv\"]\n",
    "# strains to analyze\n",
    "search_keys = [\"WT\", \"A3B3C3\", \"KaiC3\", \"KaiA3_\", \"KaiA3B3_\", \"KaiA3C3\", \"KaiB3C3\"]\n",
    "# calculating the first period for each strain\n",
    "for key in search_keys:\n",
    "    calculate_first_period(search_key=key, files=smoothed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_avg_sd(data:pd.DataFrame, sort_key:str) -> tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Function for sorting the output DataFrame from the 'calculate_first_period' function\n",
    "    for one strain of choice and calculating the mean and standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    # sorting\n",
    "    filtered = data[sorted([key for key in data.keys() if re.search(sort_key, key) and not re.search(r\"Wilde\", key)])]\n",
    "    # calculating the mean\n",
    "    avg = np.mean(filtered, axis=1)\n",
    "    # calculating the standard deviation with n - 1 degrees of freedom\n",
    "    sd = np.std(filtered, axis=1, ddof=1)\n",
    "    return avg, sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot_4x3(color_dic:dict, dimensions:tuple, lw:int=1.5) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function for plotting the data from the first experiment and the normalized data from all three experiments to a\n",
    "    4x2 subplot.\n",
    "    \"\"\"\n",
    "\n",
    "    def set_axis(ax:plt.Axes, xlim:tuple, ylim:tuple, xticks_major:list, xticks_minor:list,\n",
    "                 yticks_major:list, yticks_minor:list, xlabel:str, ylabel:str) -> None:\n",
    "        \"\"\"\n",
    "        Function for formating the x- and y-axes.\n",
    "        \"\"\"\n",
    "        \n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "        ax.set_xticks(xticks_major)\n",
    "        ax.set_xticklabels(xticks_major)\n",
    "        ax.set_xticks(xticks_minor, minor=True)\n",
    "        ax.set_xticklabels(xticks_minor, minor=True)\n",
    "\n",
    "        ax.set_yticks(yticks_major)\n",
    "        ax.set_yticklabels(yticks_major)\n",
    "        ax.set_yticks(yticks_minor, minor=True)\n",
    "        # ax.set_yticklabels(yticks_minor, minor=True)\n",
    "\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=14, length=5)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=12, length=3)\n",
    "\n",
    "        if xlabel == \"off\":\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_xticklabels([], minor=True)\n",
    "        else:\n",
    "            ax.set_xlabel(xlabel, fontsize=16)\n",
    "\n",
    "        if ylabel == \"off\":\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_yticklabels([], minor=True)\n",
    "        else:\n",
    "            ax.set_ylabel(ylabel, fontsize=16)\n",
    "        \n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    ##############################################################################################\n",
    "    def plot_growth_with_sd(data:pd.DataFrame, ax:plt.Axes, search_key:str, xlabel:bool=False, ylabel:bool=False) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for plotting the raw signal from the BioLector with standard deviation and a regression curve \n",
    "        for reference. Which axis to plot to and which strain to plot can be specified with the ax and search_key argument.\n",
    "        \"\"\"\n",
    "\n",
    "        sub_data = data[[i for i in data.keys() if re.search(search_key, i)]]\n",
    "\n",
    "        x = data[\"Time(h)\"]\n",
    "        y1 = sub_data[f\"{search_key}_mean\"]\n",
    "        sd1 = sub_data[f\"{search_key}_sd\"]\n",
    "        y2 = sub_data[f\"{search_key}_reg\"]\n",
    "\n",
    "        for key in color_dic:\n",
    "            if re.search(key, search_key):\n",
    "                color = color_dic[key]\n",
    "\n",
    "        name = search_key.removeprefix(\"6803_\").removesuffix(\"_mean\").replace(\"delta\", \"\\u0394\").replace(\"_\", \" \").replace(\"K\", \"k\")\n",
    "\n",
    "        n = data[[i for i in data.keys() if re.search(search_key + r\"_rep\\d\", i)]].shape[1]\n",
    "\n",
    "        ax.plot(x, y1, color=color, lw=lw, label=f\"{name} \\u00B1 sd; n = {n}\")\n",
    "        ax.fill_between(x, y1+sd1, y1-sd1, color=color, alpha=0.4)\n",
    "        ax.plot(x, y2, color=\"k\", alpha=1, lw=1.5, ls=\"--\", label=\"Polynomial Regression\")\n",
    "\n",
    "    #########################################################################################\n",
    "\n",
    "    #########################################################################################    \n",
    "    def plot_signal_with_sd(data:pd.DataFrame, ax:plt.Axes, search_key:str, experiment:str) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for plotting the isolated and smoothed oscillation signal with standard deviation\n",
    "        agains the time. Which axis to plot to and which strains to plot can be specified with the ax and search_key argument.\n",
    "        \"\"\"\n",
    "        sub_data = data[[i for i in data.keys() if re.search(search_key, i)\n",
    "                        and (re.search(r\"mean\", i)\n",
    "                        or re.search(r\"sd\", i))]]\n",
    "\n",
    "\n",
    "        x = data[\"Time(h)\"]\n",
    "\n",
    "\n",
    "        for key in sorted([j.removesuffix(\"_mean\") for j in sub_data.keys() if j.endswith(\"_mean\")]):\n",
    "            y = sub_data[key + \"_mean\"]\n",
    "            sd = sub_data[key + \"_sd\"]\n",
    "\n",
    "            for key2 in color_dic:\n",
    "                if re.search(key2, key):\n",
    "                    color = color_dic[key2]\n",
    "        \n",
    "            n = len(experiment_keys[experiment][key])\n",
    "            \n",
    "            name = key.removeprefix(\"6803_\").removesuffix(\"_smoothed\").replace(\"delta\", \"\\u0394\").replace(\"d\", \"\\u0394\").replace(\"K\", \"k\")\n",
    "            ax.fill_between(x, y-sd, y+sd, color=color, alpha=0.4)\n",
    "            ax.plot(x, y,color=color, lw=1.5, label=f\"{name} \\u00B1 sd\\nn = {n}\")\n",
    "\n",
    "\n",
    "    #########################################################################################\n",
    "        \n",
    "    #########################################################################################\n",
    "    def first_period_boxplot(ax:plt.Axes) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Function to make a boxplot for the first periods from all replicates of the WT (control) and all strains that \n",
    "        exhibit a possibly damped backscatter phenotype.\n",
    "        \"\"\"\n",
    "\n",
    "        # loading the data from csv files\n",
    "        WT = pd.read_csv(\"..\\\\data\\\\WT_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "        A3B3C3 = pd.read_csv(\"..\\\\data\\\\A3B3C3_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "        C3 = pd.read_csv(\"..\\\\data\\\\KaiC3_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "        A3 = pd.read_csv(\"..\\\\data\\\\KaiA3__all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "        A3B3 = pd.read_csv(\"..\\\\data\\\\KaiA3B3__all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "        A3C3 = pd.read_csv(\"..\\\\data\\\\KaiA3C3_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "        B3C3 = pd.read_csv(\"..\\\\data\\\\KaiB3C3_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "\n",
    "        boxplot_list = [WT, A3B3C3, C3, A3, A3B3, A3C3, B3C3]\n",
    "        color_List = [\"forestgreen\", \"dodgerblue\", \"darkorange\", \"brown\", \"olive\", \"lawngreen\", \"mediumturquoise\"]\n",
    "\n",
    "\n",
    "        def make_boxplot(data:pd.Series, ax:plt.Axes, pos:int) -> None:        \n",
    "            \"\"\"\n",
    "            Function that takes the data from one strain, a plt.Axes object and a position on the x-axis and\n",
    "            makes a boxplot with median and mean.\n",
    "            \"\"\"\n",
    "            ax.boxplot(x=data, positions=[pos], showfliers=False, widths=[0.4], medianprops={\"color\": \"firebrick\", \"lw\": 1.5, \"zorder\": 1},\n",
    "                    showmeans=True, meanline=True, meanprops={\"color\": \"navy\", \"ls\": \"--\", \"lw\": 1.5})\n",
    "            return\n",
    "        \n",
    "        def make_scatterplot(data:pd.Series, ax:plt.Axes, color:str, pos:int, marker:str) -> str:\n",
    "            \"\"\"\n",
    "            Function for making a scatterplot from the first period data for each strain.\n",
    "            \"\"\"\n",
    "            name = data.keys()[0].split(\"_\")[2]\n",
    "            ax.scatter(x=np.repeat(pos, len(data)), y=data, color=color, label=f\"{name}; n={len(data)}\", s=40, marker=marker, edgecolors=\"k\", lw=0.5)\n",
    "            return name\n",
    "        \n",
    "        names = []\n",
    "        # looping through the list of DataFrames\n",
    "        for i, strain in enumerate(boxplot_list):\n",
    "            # setting the color for each strain\n",
    "            color = color_List[i]\n",
    "            # making the boxplot for each strain\n",
    "            make_boxplot(strain, ax, pos=i)\n",
    "\n",
    "            # making a scatterplot for experiment 1 ontop of the boxplot\n",
    "            exp1 = strain[[key for key in strain.keys() if re.search(r\"E007\", key)]]\n",
    "            name = make_scatterplot(exp1, ax, color=color, pos=i-0.1, marker=\"o\")\n",
    "            \n",
    "            # scatterplot for experiment 2\n",
    "            exp2 = strain[[key for key in strain.keys() if re.search(r\"E008\", key)]]\n",
    "            name = make_scatterplot(exp2, ax, color=color, pos=i, marker=\"^\")\n",
    "\n",
    "            # scatterplot for experiment 3, A3B3C3 is excluded sinc it wasn't in experiment 3\n",
    "            if not re.search(r\"A3B3C3\", strain.keys()[0]):\n",
    "                exp3 = strain[[key for key in strain.keys() if re.search(r\"E011\", key)]]\n",
    "                name = make_scatterplot(exp3, ax, color=color, pos=i+0.1, marker=\"s\")\n",
    "\n",
    "            names.append(name) \n",
    "\n",
    "        names = [name.replace(\"delta\", \"\\u0394\") for name in names]\n",
    "\n",
    "\n",
    "        ax.set_ylim(18, 30)\n",
    "        ax.set_yticks(range(18, 31, 4), minor=False)\n",
    "        ax.set_yticks(range(18, 31, 2), minor=True)\n",
    "        ax.set_ylabel(\"$1^{st}$ Period [h]\", fontsize=16)\n",
    "\n",
    "        lengths = [df.shape[0] for df in boxplot_list]\n",
    "        labels = []\n",
    "\n",
    "        for label, length in zip(names, lengths):\n",
    "            labels.append(f\"{label}\\nn={length}\")\n",
    "\n",
    "\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", length=5, labelsize=14)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", length=3)\n",
    "\n",
    "        ax.set_xticklabels(labels, fontsize=16, rotation=0, horizontalalignment=\"center\")\n",
    "\n",
    "\n",
    "    ###############################################################################################\n",
    "\n",
    "    ###############################################################################################\n",
    "    def smoothed_and_fit(ax:plt.Axes, data:pd.DataFrame, data_average:pd.Series, data_sd:pd.Series,\n",
    "                        color:str, name:str, period:tuple, phi:tuple) -> None:\n",
    "        \"\"\"\n",
    "        Function for plotting the smoothed oscillation signal and a cosine fit to the axis given.\n",
    "        \"\"\"\n",
    "\n",
    "        def cos_func(x, A, omega, phi):\n",
    "            \"\"\"\n",
    "            Function for a harmonic oscillation.\n",
    "            \"\"\"\n",
    "            return A*np.cos(omega*x+phi)\n",
    "\n",
    "\n",
    "        def fit_cos(ax:plt.Axes, data:pd.DataFrame, data_average:pd.Series,\n",
    "                    period_l:int, period_u:int, phi_l:int, phi_u:int) -> None:\n",
    "            \"\"\"\n",
    "            Function to fit the oscillation signal with the cos_func\n",
    "            \"\"\"\n",
    "            # estimate for the amplitude parameter\n",
    "            A = (max(data_average.dropna()) - np.mean(data_average.dropna()))\n",
    "            # estimate for the lower boundary of the period\n",
    "            omega_l = 2*np.pi/period_l\n",
    "            # estimate for the upper boundary of the period\n",
    "            omega_u = 2*np.pi/period_u\n",
    "\n",
    "            # fitting function\n",
    "            popt, pcov = curve_fit(f=cos_func, xdata=data[\"Time[h]\"].dropna(), ydata=data_average.dropna(),\n",
    "                                bounds=([0, omega_u, phi_l], [A, omega_l, phi_u]))\n",
    "\n",
    "            # calculating the period from the angular velocity\n",
    "            calc_period = 1/popt[1]*2*np.pi\n",
    "\n",
    "            # plotting the data to an axis object\n",
    "            ax.plot(data[\"Time[h]\"], cos_func(data[\"Time[h]\"], popt[0], popt[1], popt[2]), c=\"k\", ls=\":\", label=\"fit\", lw=1.5)\n",
    "\n",
    "            return\n",
    "    \n",
    "        period_l = period[0]\n",
    "        period_u = period[1]\n",
    "\n",
    "        # boundaries for the phase angle\n",
    "        phi_l = (2*np.pi)/phi[1]\n",
    "        phi_u = (2*np.pi)/phi[0]\n",
    "        \n",
    "        # plotting the oscillation signal with sd first\n",
    "        ax.plot(data[\"Time[h]\"], data_average, color=color, label=f\"{name} \\u00B1 sd\", lw=1.5)\n",
    "        ax.fill_between(data[\"Time[h]\"], data_average+data_sd, data_average-data_sd, color=color, alpha=0.3)\n",
    "\n",
    "        # fitting function\n",
    "        fit_cos(ax, data, data_average, period_l, period_u, phi_l, phi_u)\n",
    "\n",
    "\n",
    "        ax.set_yticks(range(-6, 7, 2), minor=False)\n",
    "        ax.set_yticks(range(-7, 8, 1), minor=True)\n",
    "\n",
    "        ax.set_xticks(range(0, 84, 24), minor=False)\n",
    "        ax.set_xticks(range(0, 84, 12), minor=True)\n",
    "\n",
    "        ax.set_xticklabels(list(range(0, 84, 12)), minor=True)\n",
    "\n",
    "\n",
    "        return\n",
    "#############################################################################################################\n",
    "        \n",
    "    # data of the first experiment with the mean and std beeing taken after the regression analysis\n",
    "    E007_data_avg_last = pd.read_csv(\"..\\\\data\\\\E007_total_data_mean_after.csv\", index_col=0)\n",
    "    # data of the first experiment with mean and std beeing taken befor the regression analysis\n",
    "    E007_data_avg_first = pd.read_csv(\"..\\\\data\\\\E007_total_data_mean_first.csv\", index_col=0)\n",
    "\n",
    "    # data of the 1st peak phase shift and relative amplitude of all three experiments\n",
    "    E007 = pd.read_csv(\"..\\\\data\\\\E007_peak_stats.csv\", index_col=0)\n",
    "    E008 = pd.read_csv(\"..\\\\data\\\\E008_peak_stats.csv\", index_col=0)\n",
    "    E011 = pd.read_csv(\"..\\\\data\\\\E011_peak_stats.csv\", index_col=0)\n",
    "\n",
    "    # oscillation signal data for the first 2 experiments\n",
    "    E007_smooth = pd.read_csv(\"..\\\\data\\\\E007_smoothed_data.csv\", index_col=0)\n",
    "    E008_smooth = pd.read_csv(\"..\\\\data\\\\E008_smoothed_data.csv\", index_col=0)\n",
    "\n",
    "    # sorted data for WT, dkaiA1B1C1 and dkaiA3B3C3 from the 1. experiment and dkaiA3B3C3 from the second\n",
    "    E007_WT_average, E007_WT_std = sort_avg_sd(E007_smooth, \"WT\")\n",
    "    E007_A1B1C1_average, E007_A1B1C1_std = sort_avg_sd(E007_smooth, \"A1B1C1\")\n",
    "    E007_A3B3C3_average, E007_A3B3C3_std = sort_avg_sd(E007_smooth, \"A3B3C3\")\n",
    "    E008_A3B3C3_average, E008_A3B3C3_std = sort_avg_sd(E008_smooth, \"A3B3C3\")\n",
    "\n",
    "    # list with DataFrames to iterate over\n",
    "    data_list = [E007, E008, E011]\n",
    "\n",
    "    # list of the search keys to use for the plotting function\n",
    "    search_keys = [\"6803_WT\", \"6803_deltaKaiA1B1C1\", \"6803_deltaKaiA3B3C3\", r\"A1B1C1|A3B3C3|WT\",  r\"C3_\", r\"A3B3_|A3_|KaiC3_|B3_\"]\n",
    "\n",
    "    fig = plt.figure(figsize=dimensions) # , layout=\"constrained\"\n",
    "\n",
    "\n",
    "    gs1 = GridSpec(4, 3, figure=fig)\n",
    "    gs2 = gs1[2, 2].subgridspec(1, 7)\n",
    "    gs3 = gs1[1, 2].subgridspec(2, 1)\n",
    "    ax1 = fig.add_subplot(gs1[0,0])\n",
    "    ax2 = fig.add_subplot(gs1[0,1])\n",
    "    ax3 = fig.add_subplot(gs1[0,2])\n",
    "    ax4 = fig.add_subplot(gs1[1,0])\n",
    "    ax5 = fig.add_subplot(gs1[1,1])\n",
    "\n",
    "    ax6 = fig.add_subplot(gs3[0])\n",
    "    ax7 = fig.add_subplot(gs3[1])\n",
    "\n",
    "    ax8 = fig.add_subplot(gs1[2,0])\n",
    "    ax9 = fig.add_subplot(gs1[2,1])\n",
    "\n",
    "    ax10 = fig.add_subplot(gs2[0, 0:5]) \n",
    "    ax11 = fig.add_subplot(gs2[0,5:7])\n",
    "\n",
    "    ax12 = fig.add_subplot(gs1[3,0:2])\n",
    "\n",
    "    ax13 = fig.add_subplot(gs1[3,2])\n",
    "\n",
    "    xticks_major = [i for i in range(0, 85, 24)]\n",
    "    xticks_minor = [i for i in range(0, 85, 12)]\n",
    "\n",
    "    # using the raw signal plotting function on the first three axes\n",
    "    plot_growth_with_sd(E007_data_avg_first, ax1, search_keys[0])\n",
    "    set_axis(ax1, (0, 84), (100, 400), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(100, 401, 100)), list(range(100, 401, 50)), \"Time [h]\", \"Backscatter [a.u.]\")\n",
    "    plot_growth_with_sd(E007_data_avg_first, ax2, search_keys[1])\n",
    "    set_axis(ax2, (0, 84), (100, 400), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(100, 401, 100)), list(range(100, 401, 50)), \"Time [h]\", \"Backscatter [a.u.]\")\n",
    "    plot_growth_with_sd(E007_data_avg_first, ax3, search_keys[2])\n",
    "    set_axis(ax3, (0, 84), (100, 400), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(100, 401, 100)), list(range(100, 401, 50)), \"Time [h]\", \"Backscatter [a.u.]\")\n",
    "    \n",
    "\n",
    "    # plotting the smoothed oscillation signal with a cos-fit to the next 4 axes\n",
    "    smoothed_and_fit(ax4, E007_smooth, E007_WT_average, E007_WT_std, \"forestgreen\", \"WT\", (18, 30), (1, 12))\n",
    "    set_axis(ax4, (0, 84), (-7, 7), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(-6, 7, 2)), list(range(-7, 8, 1)), \"Time [h]\", \"Relative\\nBackscatter [a.u.]\")\n",
    "    \n",
    "    smoothed_and_fit(ax5, E007_smooth, E007_A1B1C1_average, E007_A1B1C1_std, \"orchid\", \"$\\Delta$A1B1C1\", (28, 40), (1, 12))\n",
    "    set_axis(ax5, (0, 84), (-7, 7), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(-6, 7, 2)), list(range(-7, 8, 1)), \"Time [h]\", \"Relative\\nBackscatter [a.u.]\")\n",
    "    \n",
    "    smoothed_and_fit(ax6, E007_smooth, E007_A3B3C3_average, E007_A3B3C3_std, \"dodgerblue\", \"$\\Delta$A3B3C3\", (18, 26), (1, 12))\n",
    "    set_axis(ax6, (0, 84), (-3, 3), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(-2, 3, 2)), list(range(-3, 4, 1)), \"off\", \" \")\n",
    "    \n",
    "    smoothed_and_fit(ax7, E008_smooth, E008_A3B3C3_average, E008_A3B3C3_std, \"dodgerblue\", \"$\\Delta$A3B3C3\", (18, 26), (1, 12))\n",
    "    set_axis(ax7, (0, 84), (-3, 3), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(-2, 3, 2)), list(range(-3, 4, 1)), \"Time [h]\", \" \")\n",
    "    ax7.set_ylabel(\"Relative\\nBackscatter [a.u.]\", fontsize=16, y=1.11)\n",
    "\n",
    "\n",
    "    # using the oscillation signal plotting function on axes 8 & 9\n",
    "    plot_signal_with_sd(E007_data_avg_last, ax8, search_keys[4], \"E007\")\n",
    "    set_axis(ax8, (0, 84), (-7, 7), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(-6, 7, 2)), list(range(-7, 8, 1)), \"Time [h]\", \"Relative\\nBackscatter [a.u.]\")\n",
    "    plot_signal_with_sd(E007_data_avg_last, ax9, search_keys[5], \"E007\")\n",
    "    set_axis(ax9, (0, 84), (-7, 7), list(range(0, 85, 24)), list(range(0, 85, 12)), \n",
    "             list(range(-6, 7, 2)), list(range(-7, 8, 1)), \"Time [h]\", \"Relative\\nBackscatter [a.u.]\")\n",
    "\n",
    "    first_period_boxplot(ax12)\n",
    "\n",
    "\n",
    "    ######################################################################################################\n",
    "    # plot of the phase shift and relative amplitude of the first peak of all three experiments with \n",
    "    # discontinous x-axis:\n",
    "\n",
    "    marker_list = [\"o\", \"^\", \"s\"]\n",
    "    exp_list = [\"E007\", \"E008\", \"E011\"]\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        ordered_data = data_list[i]\n",
    "        marker = marker_list[i]\n",
    "        exp = exp_list[i]\n",
    "\n",
    "        marker_scale = []\n",
    "        for key in ordered_data.keys():\n",
    "            n = 1.5 * int(key.split(\"=\")[1])\n",
    "            marker_scale.append(n)\n",
    "\n",
    "        x = ordered_data.loc[\"ratio_height_to_WT\"]\n",
    "        xerr = np.array(ordered_data.loc[\"ratio_error\"] * x)\n",
    "\n",
    "        y = ordered_data.loc[\"difference_timepoint_to_WT\"]\n",
    "        yerr = ordered_data.loc[\"difference_error\"]\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            \n",
    "            for key2 in color_dic:\n",
    "                if re.search(key2, ordered_data.keys()[i]):\n",
    "                    color = color_dic[key2]\n",
    "\n",
    "            label = f\"{exp} \" + ordered_data.keys()[i].removeprefix(\"6803_\").replace(\"_\", \"; \").replace(\"delta\", \"\\u0394\").replace(\"K\", \"k\")\n",
    "            plot = ax10.errorbar(x[i], y[i], xerr=xerr[i], yerr=yerr[i], fmt=marker, ls=\"\", markersize=marker_scale[i], color=color,\n",
    "                        label=label, capsize=3, mec=\"k\", mew=0.5)\n",
    "            [bar.set_alpha(0.6) for bar in plot[2]]\n",
    "            [bar.set_alpha(0.6) for bar in plot[1]]\n",
    "            plot = ax11.errorbar(x[i], y[i], xerr=xerr[i], yerr=yerr[i], fmt=marker, ls=\"\", markersize=marker_scale[i], color=color,\n",
    "                label=label, capsize=3, mec=\"k\", mew=0.5)\n",
    "            [bar.set_alpha(0.6) for bar in plot[2]]\n",
    "            [bar.set_alpha(0.6) for bar in plot[1]]\n",
    "\n",
    "        ax10.set_ylabel(\"Phase Shift $1^{st}$ Peak [h]\", fontsize=16)\n",
    "        ax10.set_ylim(-10, 8)\n",
    "        ax11.set_ylim(-10, 8)\n",
    "        ax10.set_yticks(range(-10, 9, 2), minor=False)\n",
    "        ax10.set_yticklabels(range(-10, 9, 2), minor=False)\n",
    "        ax10.set_yticks(range(-10, 9, 1), minor=True)\n",
    "\n",
    "\n",
    "        ax10.set_xlabel(\"Relative Amplitude $1^{st}$ Peak [a.u.] \", fontsize=16, x=0.75)\n",
    "        ax10.set_xlim([0, .5])\n",
    "        ax11.set_xlim([0.9, 1.1])\n",
    "        ax10.set_xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5], minor=False)\n",
    "        ax10.set_xticklabels([0, 0.1, 0.2, 0.3, 0.4, 0.5], minor=False, ha=\"right\")\n",
    "        ax10.set_xticks([0.05, 0.15, 0.25, 0.35, 0.45], minor=True)\n",
    "        ax11.set_xticks([0.9, 1.0, 1.1], minor=False)\n",
    "        ax11.set_xticklabels([0.9, 1.0, 1.1], minor=False, ha=\"left\")\n",
    "        ax11.set_xticks([0.95, 1.05], minor=True)\n",
    "        ax11.set_yticklabels([])\n",
    "\n",
    "        ax10.tick_params(axis=\"both\", length=5, labelsize=14)\n",
    "        ax10.tick_params(axis=\"both\", length=3, labelsize=12, which=\"minor\")\n",
    "        ax11.tick_params(axis=\"x\", length=5, labelsize=14)\n",
    "        ax11.tick_params(axis=\"x\", length=3, labelsize=12, which=\"minor\")\n",
    "\n",
    "        ax10.spines['right'].set_visible(False)\n",
    "        ax11.spines['left'].set_visible(False)\n",
    "        ax11.tick_params(axis=\"y\", which=\"both\", left=False)\n",
    "\n",
    "    ##############################################################################################################\n",
    "\n",
    "\n",
    "    # plot legend to last subplot\n",
    "\n",
    "    custom_lines1 = [Line2D([0], [0], color=\"forestgreen\", lw=4),\n",
    "                    Line2D([0], [0], color=\"orchid\", lw=4),\n",
    "                    Line2D([0], [0], color=\"dodgerblue\", lw=4),\n",
    "                    Line2D([0], [0], color=\"darkorange\", lw=4),\n",
    "                    Line2D([0], [0], color=\"lawngreen\", lw=4),\n",
    "                    Line2D([0], [0], color=\"mediumturquoise\", lw=4),\n",
    "                    Line2D([0], [0], color=\"brown\", lw=4),\n",
    "                    Line2D([0], [0], color=\"olive\", lw=4),\n",
    "                    Line2D([0], [0], color=\"crimson\", lw=4),\n",
    "                    Line2D([0], [0], color=\"k\", lw=2, ls=\"--\", alpha=1),\n",
    "                    Line2D([0], [0], color=\"k\", lw=2, ls=\":\", alpha=1)]\n",
    "\n",
    "    custom_labels1 = [\"WT\", \"$\\Delta$kaiA1B1C1\", \"$\\Delta$kaiA3B3C3\", \"$\\Delta$kaiC3\", \"$\\Delta$kaiA3C3\", \"$\\Delta$kaiB3C3\", \"$\\Delta$kaiA3\",\n",
    "                    \"$\\Delta$kaiA3B3\", \"$\\Delta$kaiB3\", \"Polynomial\\nRegression\", \"Cosine Fit\"]\n",
    "\n",
    "    custom_lines2 = [Line2D([0], [0], marker=\"o\", color=\"grey\", ms=10, ls=\"\"),\n",
    "                    Line2D([0], [0], marker=\"^\", color=\"grey\", ms=10, ls=\"\"),\n",
    "                    Line2D([0], [0], marker=\"s\", color=\"grey\", ms=10, ls=\"\")]\n",
    "\n",
    "    custom_labels2 = [\"Experiment 1\", \"Experiment 2\", \"Experiment 3\"]\n",
    "\n",
    "    ax13.legend(frameon=False, handles=custom_lines1, labels=custom_labels1, loc=9, fontsize=15, ncol=2, bbox_to_anchor=((0.5, 0.9)))\n",
    "\n",
    "    ax14 = ax13.twinx()\n",
    "    ax14.legend(frameon=False, handles=custom_lines2, labels=custom_labels2, loc=9, fontsize=15, ncol=1, bbox_to_anchor=((0.5, 0.3)))\n",
    "\n",
    "    ax13.set_axis_off()\n",
    "    ax14.set_axis_off()\n",
    "\n",
    "    # fig.text(0, 1, \"a\", fontsize=20, horizontalalignment=\"left\", verticalalignment=\"top\")\n",
    "    # fig.text(0.3333, 1, \"b\", fontsize=20, horizontalalignment=\"left\", verticalalignment=\"top\")\n",
    "    # fig.text(0.6666, 1, \"c\", fontsize=20, horizontalalignment=\"left\", verticalalignment=\"top\")\n",
    "    # fig.text(0, 0.6666, \"d\", fontsize=20, horizontalalignment=\"left\", verticalalignment=\"top\")\n",
    "    # fig.text(0.3333, 0.6666, \"e\", fontsize=20, horizontalalignment=\"left\", verticalalignment=\"top\")\n",
    "    # fig.text(0.6666, 0.6666, \"f\", fontsize=20, horizontalalignment=\"left\", verticalalignment=\"top\")\n",
    "    # fig.text(0, 0.3333, \"g\", fontsize=20, horizontalalignment=\"left\", verticalalignment=\"top\")\n",
    "    # fig.text(0.3333, 0.3333, \"h\", fontsize=20, horizontalalignment=\"left\", verticalalignment=\"top\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(\"..\\\\plots\\\\multiplot_4x3.png\", dpi=500)\n",
    "    # fig.savefig(\"..\\\\plots\\\\multiplot_4x3.tif\", dpi=500)\n",
    "    fig.savefig(\"..\\\\plots\\\\multiplot_4x3.pdf\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot_4x3(color_dic=color_dic, dimensions=(20, 20), lw=lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_params(ax:plt.Axes, data:pd.DataFrame, data_average:pd.Series, data_sd:pd.Series,\n",
    "                     color:str, name:str, y_lim:tuple, period:tuple, phi:tuple) -> None:\n",
    "    \"\"\"\n",
    "    Cosine fit function for supplement figure\n",
    "    \"\"\"\n",
    "    def cos_func(x, A, omega, phi):\n",
    "        return A*np.cos(omega*x+phi)\n",
    "        \n",
    "\n",
    "    def fit_cos(ax:plt.Axes, data:pd.DataFrame, data_average:pd.Series,\n",
    "                period_l:int, period_u:int, phi_l:int, phi_u:int) -> None:\n",
    "\n",
    "        A = (max(data_average.dropna()) - np.mean(data_average.dropna()))\n",
    "        omega_l = 2*np.pi/period_l\n",
    "        omega_u = 2*np.pi/period_u\n",
    "\n",
    "        popt, pcov = curve_fit(f=cos_func, xdata=data[\"Time[h]\"].dropna(), ydata=data_average.dropna(),\n",
    "                               bounds=([0, omega_u, phi_l], [A, omega_l, phi_u]))\n",
    "\n",
    "\n",
    "        ax.plot(data[\"Time[h]\"], cos_func(data[\"Time[h]\"], popt[0], popt[1], popt[2]), c=\"k\", ls=\":\", label=\"fit\")\n",
    "\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    period_l = period[0]\n",
    "    period_u = period[1]\n",
    "    \n",
    "    ylim_l = y_lim[0]\n",
    "    ylim_u = y_lim[1]\n",
    "\n",
    "    phi_l = (2*np.pi)/phi[1]\n",
    "    phi_u = (2*np.pi)/phi[0]\n",
    "    \n",
    "    ax.plot(data[\"Time[h]\"], data_average, color=color)\n",
    "    ax.fill_between(data[\"Time[h]\"], data_average+data_sd, data_average-data_sd, color=color, alpha=0.3)\n",
    "\n",
    "    fit_cos(ax, data, data_average, period_l, period_u, phi_l, phi_u)#\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ax.set_title(f\"{name}\", fontsize=14)\n",
    "\n",
    "    ax.set_xlim(0, 84)\n",
    "\n",
    "    ax.set_yticks(range(-6, 7, 2), minor=False)\n",
    "    ax.set_yticks(range(-7, 8, 1), minor=True)\n",
    "\n",
    "    ax.set_xticks(range(0, 85, 24), minor=False)\n",
    "    ax.set_xticks(range(0, 85, 12), minor=True)\n",
    "\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "\n",
    "\n",
    "    ax.set_ylim(ylim_l, ylim_u)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplement figure S10\n",
    "\n",
    "fig, ((ax1, ax4, ax7, ax10, ax13, ax16, ax19, ax22),\n",
    "      (ax2, ax5, ax8, ax11, ax14, ax17, ax20, ax23),\n",
    "      (ax3, ax6, ax9, ax12, ax15, ax18, ax21, ax24)) = plt.subplots(ncols=8, nrows=3,\n",
    "                                                                    figsize=(18, 8), layout=\"constrained\",\n",
    "                                                                    sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "E007 = pd.read_csv(\"..\\\\data\\\\E007_smoothed_data.csv\", index_col=0)\n",
    "E008 = pd.read_csv(\"..\\\\data\\\\E008_smoothed_data.csv\", index_col=0)\n",
    "E011 = pd.read_csv(\"..\\\\data\\\\E011_smoothed_data.csv\", index_col=0)\n",
    "\n",
    "# WT data\n",
    "E007_WT_avg, E007_WT_sd = sort_avg_sd(E007, r\"WT\")\n",
    "E008_WT_avg, E008_WT_sd = sort_avg_sd(E008, r\"WT\")\n",
    "E011_WT_avg, E011_WT_sd = sort_avg_sd(E011, r\"WT\")\n",
    "\n",
    "# A1B1C1 data\n",
    "E007_A1B1C1_avg, E007_A1B1C1_sd = sort_avg_sd(E007, r\"KaiA1B1C1\")\n",
    "E008_A1B1C1_avg, E008_A1B1C1_sd = sort_avg_sd(E008, r\"KaiA1B1C1\")\n",
    "E011_A1B1C1_avg, E011_A1B1C1_sd = sort_avg_sd(E011, r\"KaiA1B1C1\")\n",
    "\n",
    "# A3 data\n",
    "E007_A3_avg, E007_A3_sd = sort_avg_sd(E007, r\"KaiA3_\")\n",
    "E008_A3_avg, E008_A3_sd = sort_avg_sd(E008, r\"KaiA3_\")\n",
    "E011_A3_avg, E011_A3_sd = sort_avg_sd(E011, r\"KaiA3_\")\n",
    "\n",
    "# B3 data\n",
    "E007_B3_avg, E007_B3_sd = sort_avg_sd(E007, r\"KaiB3_\")\n",
    "E008_B3_avg, E008_B3_sd = sort_avg_sd(E008, r\"KaiB3_\")\n",
    "E011_B3_avg, E011_B3_sd = sort_avg_sd(E011, r\"KaiB3_\")\n",
    "\n",
    "# C3 data\n",
    "E007_C3_avg, E007_C3_sd = sort_avg_sd(E007, r\"KaiC3\")\n",
    "E008_C3_avg, E008_C3_sd = sort_avg_sd(E008, r\"KaiC3\")\n",
    "E011_C3_avg, E011_C3_sd = sort_avg_sd(E011, r\"KaiC3\")\n",
    "\n",
    "# A3B3 data\n",
    "E007_A3B3_avg, E007_A3B3_sd = sort_avg_sd(E007, r\"KaiA3B3_\")\n",
    "E008_A3B3_avg, E008_A3B3_sd = sort_avg_sd(E008, r\"KaiA3B3_\")\n",
    "E011_A3B3_avg, E011_A3B3_sd = sort_avg_sd(E011, r\"KaiA3B3_\")\n",
    "\n",
    "# A3C3 data\n",
    "E007_A3C3_avg, E007_A3C3_sd = sort_avg_sd(E007, r\"KaiA3C3\")\n",
    "E008_A3C3_avg, E008_A3C3_sd = sort_avg_sd(E008, r\"KaiA3C3\")\n",
    "E011_A3C3_avg, E011_A3C3_sd = sort_avg_sd(E011, r\"KaiA3C3\")\n",
    "\n",
    "# B3C3 data\n",
    "E007_B3C3_avg, E007_B3C3_sd = sort_avg_sd(E007, r\"KaiB3C3_\")\n",
    "E008_B3C3_avg, E008_B3C3_sd = sort_avg_sd(E008, r\"KaiB3C3_\")\n",
    "E011_B3C3_avg, E011_B3C3_sd = sort_avg_sd(E011, r\"KaiB3C3_\")\n",
    "\n",
    "# WT fits \n",
    "fit_and_params(ax1, E007, E007_WT_avg, E007_WT_sd, \"forestgreen\", \"Exp. 1 WT\",\n",
    "               (-7, 7), (18, 30), (1, 12))\n",
    "fit_and_params(ax2, E008, E008_WT_avg, E008_WT_sd, \"forestgreen\", \"Exp. 2 WT\",\n",
    "               (-7, 7), (18, 30), (1, 12))\n",
    "fit_and_params(ax3, E011, E011_WT_avg, E011_WT_sd, \"forestgreen\", \"Exp. 3 WT\",\n",
    "               (-7, 7), (18, 30), (1, 12))\n",
    "\n",
    "# A1B1C1 fits\n",
    "fit_and_params(ax4, E007, E007_A1B1C1_avg, E007_A1B1C1_sd, \"orchid\", \"Exp. 1 \\u0394kaiA1B1C1\",\n",
    "               (-7, 7), (28, 40), (1, 12))\n",
    "\n",
    "fit_and_params(ax5, E008, E008_A1B1C1_avg, E008_A1B1C1_sd, \"orchid\", \"Exp. 2 \\u0394kaiA1B1C1\",\n",
    "               (-7, 7), (28, 40), (1, 12))\n",
    "fit_and_params(ax6, E011, E011_A1B1C1_avg, E011_A1B1C1_sd, \"orchid\", \"Exp. 3 \\u0394kaiA1B1C1\",\n",
    "               (-7, 7), (28, 40), (1, 12))\n",
    "\n",
    "# A3 fits\n",
    "fit_and_params(ax7, E007, E007_A3_avg, E007_A3_sd, \"brown\", \"Exp. 1 \\u0394kaiA3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax8, E008, E008_A3_avg, E008_A3_sd, \"brown\", \"Exp. 2 \\u0394kaiA3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax9, E011, E011_A3_avg, E011_A3_sd, \"brown\", \"Exp. 3 \\u0394kaiA3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "\n",
    "# B3 fits\n",
    "fit_and_params(ax10, E007, E007_B3_avg, E007_B3_sd, \"crimson\", \"Exp. 1 \\u0394kaiB3\",\n",
    "               (-7, 7), (35, 50), (1, 12))\n",
    "fit_and_params(ax11, E008, E008_B3_avg, E008_B3_sd, \"crimson\", \"Exp. 2 \\u0394kaiB3\",\n",
    "               (-7, 7), (35, 50), (1, 12))\n",
    "fit_and_params(ax12, E011, E011_B3_avg, E011_B3_sd, \"crimson\", \"Exp. 3 \\u0394kaiB3\",\n",
    "               (-7, 7), (35, 50), (1, 12))\n",
    "\n",
    "# C3 fits\n",
    "fit_and_params(ax13, E007, E007_C3_avg, E007_C3_sd, \"darkorange\", \"Exp. 1 \\u0394kaiC3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax14, E008, E008_C3_avg, E008_C3_sd, \"darkorange\", \"Exp. 2 \\u0394kaiC3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax15, E011, E011_C3_avg, E011_C3_sd, \"darkorange\", \"Exp. 3 \\u0394kaiC3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "\n",
    "# A3B3 fits\n",
    "fit_and_params(ax16, E007, E007_A3B3_avg, E007_A3B3_sd, \"olive\", \"Exp. 1 \\u0394kaiA3B3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax17, E008, E008_A3B3_avg, E008_A3B3_sd, \"olive\", \"Exp. 2 \\u0394kaiA3B3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax18, E011, E011_A3B3_avg, E011_A3B3_sd, \"olive\", \"Exp. 3 \\u0394kaiA3B3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "\n",
    "# A3C3 fits\n",
    "fit_and_params(ax19, E007, E007_A3C3_avg, E007_A3C3_sd, \"lawngreen\", \"Exp. 1 \\u0394kaiA3C3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax20, E008, E008_A3C3_avg, E008_A3C3_sd, \"lawngreen\", \"Exp. 2 \\u0394kaiA3C3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax21, E011, E011_A3C3_avg, E011_A3C3_sd, \"lawngreen\", \"Exp. 3 \\u0394kaiA3C3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "\n",
    "# B3C3 fits\n",
    "fit_and_params(ax22, E007, E007_B3C3_avg, E007_B3C3_sd, \"mediumturquoise\", \"Exp. 1 \\u0394kaiB3C3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax23, E008, E008_B3C3_avg, E008_B3C3_sd, \"mediumturquoise\", \"Exp. 2 \\u0394kaiB3C3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "fit_and_params(ax24, E011, E011_B3C3_avg, E011_B3C3_sd, \"mediumturquoise\", \"Exp. 3 \\u0394kaiB3C3\",\n",
    "               (-7, 7), (18, 26), (1, 12))\n",
    "\n",
    "\n",
    "ax1.set_ylabel(\"Rel. Backscatter [a.u.]\", fontsize=14)\n",
    "ax2.set_ylabel(\"Rel. Backscatter [a.u.]\", fontsize=14)\n",
    "ax3.set_ylabel(\"Rel. Backscatter [a.u.]\", fontsize=14)\n",
    "\n",
    "ax3.set_xlabel(\"Time [h]\", fontsize=14)\n",
    "ax6.set_xlabel(\"Time [h]\", fontsize=14)\n",
    "ax9.set_xlabel(\"Time [h]\", fontsize=14)\n",
    "ax12.set_xlabel(\"Time [h]\", fontsize=14)\n",
    "ax15.set_xlabel(\"Time [h]\", fontsize=14)\n",
    "ax18.set_xlabel(\"Time [h]\", fontsize=14)\n",
    "ax21.set_xlabel(\"Time [h]\", fontsize=14)\n",
    "ax24.set_xlabel(\"Time [h]\", fontsize=14)\n",
    "\n",
    "fig.savefig(\"..\\\\plots\\\\signal+fits.png\", dpi=500)\n",
    "fig.savefig(\"..\\\\plots\\\\signal+fits.pdf\", dpi=500)\n",
    "# fig.savefig(\"..\\\\plots\\\\signal+fits.tif\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params(data:pd.DataFrame, data_average:pd.Series,\n",
    "           name:str, period:tuple, phi:tuple) -> tuple[np.ndarray, float, str]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to fit a cos function to the oscillation data and return the parameters and period\n",
    "    \"\"\"\n",
    "\n",
    "    def cos_func(x, A, omega, phi):\n",
    "        return A*np.cos(omega*x+phi)\n",
    "        \n",
    "\n",
    "    def fit_cos(data:pd.DataFrame, data_average:pd.Series,\n",
    "                period_l:int, period_u:int, phi_l:int, phi_u:int) -> tuple[np.ndarray, float]:\n",
    "\n",
    "        A = (max(data_average.dropna()) - np.mean(data_average.dropna()))\n",
    "        omega_l = 2*np.pi/period_l\n",
    "        omega_u = 2*np.pi/period_u\n",
    "\n",
    "        popt, pcov = curve_fit(f=cos_func, xdata=data[\"Time[h]\"].dropna(), ydata=data_average.dropna(),\n",
    "                               bounds=([0, omega_u, phi_l], [A, omega_l, phi_u]))\n",
    "\n",
    "        calc_period = 1/popt[1]*2*np.pi\n",
    "\n",
    "        return popt, calc_period\n",
    "    \n",
    "    \n",
    "    period_l = period[0]\n",
    "    period_u = period[1]\n",
    "\n",
    "    phi_l = (2*np.pi)/phi[1]\n",
    "    phi_u = (2*np.pi)/phi[0]\n",
    "\n",
    "    popt, calc_period = fit_cos(data, data_average, period_l, period_u, phi_l, phi_u)\n",
    "\n",
    "    return popt, calc_period, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a DataFrame for the parameters and period for each fit and the mean and sd for the period of each strain\n",
    "\n",
    "params_df = pd.DataFrame(index=[\"amplitude\", \"angular frequency\", \"phase angle\", \"period\", \"mean_period\", \"sd_period\"])\n",
    "\n",
    "# WT params\n",
    "popt, calc_period, name = params(E007, E007_WT_avg, \"Exp. 1 WT\", (18, 30), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "popt, calc_period, name = params(E008, E008_WT_avg, \"Exp. 2 WT\", (18, 30), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "popt, calc_period, name = params(E011, E011_WT_avg, \"Exp. 3 WT\", (18, 30), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "params_df.iloc[4, 1] = np.mean(params_df.iloc[3, 0:3])\n",
    "params_df.iloc[5, 1] = np.std(params_df.iloc[3, 0:3], ddof=1)\n",
    "\n",
    "# A1B1C1 params\n",
    "popt, calc_period, name = params(E007, E007_A1B1C1_avg, \"Exp. 1 \\u0394kaiA1B1C1\", (28, 40), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "popt, calc_period, name = params(E008, E008_A1B1C1_avg, \"Exp. 2 \\u0394kaiA1B1C1\", (28, 40), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "popt, calc_period, name = params(E011, E011_A1B1C1_avg, \"Exp. 3 \\u0394kaiA1B1C1\", (28, 40), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "params_df.iloc[4, 4] = np.mean(params_df.iloc[3, 3:6])\n",
    "params_df.iloc[5, 4] = np.std(params_df.iloc[3, 3:6], ddof=1)\n",
    "\n",
    "# B3 fits\n",
    "popt, calc_period, name = params(E007, E007_B3_avg, \"Exp. 1 \\u0394kaiB3\", (35, 50), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "popt, calc_period, name = params(E008, E008_B3_avg, \"Exp. 2 \\u0394kaiB3\", (35, 50), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "popt, calc_period, name = params(E011, E011_B3_avg, \"Exp. 3 \\u0394kaiB3\", (35, 50), (1, 12))\n",
    "params_df[name] = [*popt, calc_period, np.nan, np.nan]\n",
    "params_df.iloc[4, 7] = np.mean(params_df.iloc[3, 6:9])\n",
    "params_df.iloc[5, 7] = np.std(params_df.iloc[3, 6:9], ddof=1)\n",
    "\n",
    "params_df.to_csv(\"..\\\\data\\\\fit_params.csv\")\n",
    "params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL! unused scatterpot for the period determined by the cos fit for the simple harmonic oscillations\n",
    "\n",
    "params_df = pd.read_csv(\"..\\\\data\\\\fit_params.csv\", index_col=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), layout=\"constrained\")\n",
    "\n",
    "ax.scatter(np.repeat(1, 3), params_df.iloc[3, 0:3], color=\"forestgreen\")\n",
    "ax.hlines(np.mean(params_df.iloc[3, 0:3]), 0.9, 1.1, color=\"navy\", ls=\"--\")\n",
    "ax.text(1.2, np.mean(params_df.iloc[3, 0:3]), f\"{np.mean(params_df.iloc[3, 0:3]):.2f} h\", ha=\"left\", va=\"center\", fontsize=14)\n",
    "ax.scatter(np.repeat(2, 3), params_df.iloc[3, 3:6], color=\"orchid\")\n",
    "ax.hlines(np.mean(params_df.iloc[3, 3:6]), 1.9, 2.1, color=\"navy\", ls=\"--\")\n",
    "ax.text(1.8, np.mean(params_df.iloc[3, 3:6]), f\"{np.mean(params_df.iloc[3, 3:6]):.2f} h\", ha=\"right\", va=\"center\", fontsize=14)\n",
    "ax.scatter(np.repeat(3, 3), params_df.iloc[3, 6:9], color=\"crimson\")\n",
    "ax.hlines(np.mean(params_df.iloc[3, 6:9]), 2.9, 3.1, color=\"navy\", ls=\"--\")\n",
    "ax.text(2.8, np.mean(params_df.iloc[3, 6:9]), f\"{np.mean(params_df.iloc[3, 6:9]):.2f} h\", ha=\"right\", va=\"center\", fontsize=14)\n",
    "\n",
    "ax.set_xticks([1, 2, 3])\n",
    "ax.set_xticklabels([\"WT\", \"$\\Delta$kaiA1B1C1\", \"$\\Delta$kaiB3\"])\n",
    "\n",
    "ax.set_yticks(range(24, 43, 2))\n",
    "\n",
    "ax.tick_params(axis=\"both\", which=\"major\", length=7, labelsize=14)\n",
    "ax.set_ylabel(\"Period [h]\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fst_peak_stats() -> None:\n",
    "\n",
    "    \"\"\" function for performing an independant two-sided t-test between all strains.\n",
    "    results are saved as csv-files.\"\"\"\n",
    "\n",
    "    WT = pd.read_csv(\"..\\\\data\\\\WT_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "    A3B3C3 = pd.read_csv(\"..\\\\data\\\\A3B3C3_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "    C3 = pd.read_csv(\"..\\\\data\\\\KaiC3_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "    A3 = pd.read_csv(\"..\\\\data\\\\KaiA3__all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "    A3B3 = pd.read_csv(\"..\\\\data\\\\KaiA3B3__all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "    A3C3 = pd.read_csv(\"..\\\\data\\\\KaiA3C3_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "    B3C3 = pd.read_csv(\"..\\\\data\\\\KaiB3C3_all_experiments_first_period.csv\", index_col=0).iloc[0,:]\n",
    "\n",
    "    data_list = [WT, A3B3C3, C3, A3, A3B3, A3C3, B3C3]\n",
    "    index_list = [\"WT\", \"A3B3C3\", \"C3\", \"A3\", \"A3B3\", \"A3C3\", \"B3C3\"]\n",
    "\n",
    "    mean_median_df = pd.DataFrame(np.zeros((3, 7)), index=[\"mean\", \"median\", \"sd\"], columns=index_list)\n",
    "\n",
    "    pval_df = pd.DataFrame(np.full((7, 7), np.nan), index=index_list, columns=index_list)\n",
    "    significance_df = pd.DataFrame(np.full((7, 7), False), index=index_list, columns=index_list)\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "\n",
    "        mean = np.mean(data_list[i])\n",
    "        median = np.median(data_list[i])\n",
    "        sd = np.std(data_list[i], ddof=1)\n",
    "\n",
    "        mean_median_df.iloc[0, i] = mean\n",
    "        mean_median_df.iloc[1, i] = median\n",
    "        mean_median_df.iloc[2, i] = sd\n",
    "\n",
    "        for j in range(len(data_list)):\n",
    "            levene_res = levene(data_list[i], data_list[j])\n",
    "            levene_pval = levene_res.pvalue\n",
    "\n",
    "            # if both samples are the same:\n",
    "            if np.array_equal(data_list[i], data_list[j]):\n",
    "\n",
    "                pval_df.iloc[i,j] = np.nan\n",
    "            else:\n",
    "                # if variance differs significantly:\n",
    "                if levene_pval <= 0.05:\n",
    "                    # perform welch t-test\n",
    "                    pval = ttest_ind(data_list[i], data_list[j], equal_var=False).pvalue\n",
    "                else:\n",
    "                    # otherwise performe regular t-test\n",
    "                    pval = ttest_ind(data_list[i], data_list[j]).pvalue\n",
    "\n",
    "                if pval <= 0.05:\n",
    "                    pval_df.iloc[i,j] = pval\n",
    "                    significance_df.iloc[i,j] = True\n",
    "\n",
    "                else:\n",
    "                    pval_df.iloc[i,j] = np.nan\n",
    "\n",
    "    mean_median_df.to_csv(\"..\\\\data\\\\mean+median_boxplot.csv\")\n",
    "    pval_df.to_csv(\"..\\\\data\\\\pval_boxplot.csv\")\n",
    "    significance_df.to_csv(\"..\\\\data\\\\significance_boxplot.csv\")\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(layout=\"constrained\")\n",
    "    im = ax.imshow(pval_df, cmap=\"summer\", vmin=0, vmax=1e-3) # vmin=vmin, vmax=vmax\n",
    "\n",
    "    ax.set_xticks(np.arange(len(index_list)), labels=index_list, rotation=30)\n",
    "    ax.set_yticks(np.arange(len(index_list)), labels=index_list)\n",
    "\n",
    "    for i in range(len(index_list)):\n",
    "        for j in range(len(index_list)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if pval_df.iloc[i, j] < 0.05:\n",
    "                text = ax.text(j, i, f\"{pval_df.iloc[i, j]:.1e}\",\n",
    "                                ha=\"center\", va=\"center\", color=\"k\", fontsize=8)\n",
    "            else:\n",
    "                text = ax.text(j, i, f\"n.s.\",\n",
    "                                ha=\"center\", va=\"center\", color=\"k\", fontsize=8)\n",
    "                \n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"pvalue\", rotation=-90, va=\"bottom\")\n",
    "    ax.set_title(\"statistical comparison of 1st period\")\n",
    "\n",
    "    fig.savefig(\"..\\\\plots\\\\significance_heatmap_for_boxplot.png\", dpi=500)\n",
    "    fig.savefig(\"..\\\\plots\\\\significance_heatmap_for_boxplot.pdf\", dpi=500)\n",
    "    # fig.savefig(\"..\\\\plots\\\\significance_heatmap_for_boxplot.tif\", dpi=500)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_peak_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_signal_of_different_WTs(experiment:str, dimensions:tuple, lw:int) -> None:\n",
    "    \"\"\"\n",
    "    This function plots the raw backscatter signal of two different Synechocystis sp. PCC 6803 WTs.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(f\"..\\\\data\\\\{experiment}_renamed_data.csv\", index_col=0)\n",
    "    data = data[sorted([key for key in data.keys() if re.search(r\"Time|Wilde|WT\", key)])]\n",
    "    sub_data1 = data[[key for key in data.keys() if re.search(r\"Wilde\", key)]]\n",
    "    sub_data2 = data[[key for key in data.keys() if re.search(r\"WT\", key)\n",
    "                    and not re.search(r\"Wilde\", key)]]\n",
    "    time = data[\"Time[h]\"]\n",
    "    fig, axs = plt.subplots(1, 2, layout=\"constrained\", figsize=dimensions)\n",
    "\n",
    "    xticks_major = [i for i in range(0, 85, 24)]\n",
    "    xticks_minor = [i for i in range(0, 85, 12)]\n",
    "\n",
    "    yticks_major1 = [i for i in range(0, 4501, 1000)]\n",
    "    yticks_minor1 = [i for i in range(0, 4501, 500)]\n",
    "\n",
    "    yticks_major2 = [i for i in range(150, 451, 50)]\n",
    "    yticks_minor2 = [i for i in range(150, 451, 25)]\n",
    "\n",
    "    for i in sub_data1.keys():\n",
    "        name = i.removeprefix(\"6803_\").replace(\"_\", \" \")\n",
    "        y = sub_data1[i]\n",
    "        axs[0].plot(time, y, label=name, lw=lw)\n",
    "\n",
    "    for i in sub_data2.keys():\n",
    "        name = \"Uppsala\" + i.removeprefix(\"6803_\").replace(\"_\", \" \")\n",
    "        y = sub_data2[i]\n",
    "        axs[1].plot(time, y, label=name, lw=lw)\n",
    "\n",
    "    axs[0].set_ylabel(\"Raw Backscatter [a.u.]\", fontsize=12)\n",
    "    axs[0].set_xlabel(\"Time [h]\", fontsize=12)\n",
    "    axs[0].legend(frameon=False, fontsize=9)\n",
    "\n",
    "    axs[1].set_ylabel(\"Raw Backscatter [a.u.]\", fontsize=12)\n",
    "    axs[1].set_xlabel(\"Time [h]\", fontsize=12)\n",
    "    axs[1].legend(frameon=False, fontsize=9)\n",
    "\n",
    "    axs[0].set_xlim(0, max(time))\n",
    "    axs[0].set_xticks(xticks_major)\n",
    "    axs[0].set_xticklabels(xticks_major)\n",
    "    axs[0].set_xticks(xticks_minor, minor=True)\n",
    "    axs[0].set_xticklabels(xticks_minor, minor=True)\n",
    "\n",
    "    axs[1].set_xlim(0, max(time))\n",
    "    axs[1].set_xticks(xticks_major)\n",
    "    axs[1].set_xticklabels(xticks_major)\n",
    "    axs[1].set_xticks(xticks_minor, minor=True)\n",
    "    axs[1].set_xticklabels(xticks_minor, minor=True)\n",
    "\n",
    "    axs[0].set_ylim(0, 4500)\n",
    "    axs[0].set_yticks(yticks_major1)\n",
    "    axs[0].set_yticklabels(yticks_major1)\n",
    "    axs[0].set_yticks(yticks_minor1, minor=True)\n",
    "    #axs[0].set_yticklabels(yticks_minor1, minor=True)\n",
    "\n",
    "    axs[1].set_ylim(150, 450)\n",
    "    axs[1].set_yticks(yticks_major2)\n",
    "    axs[1].set_yticklabels(yticks_major2)\n",
    "    axs[1].set_yticks(yticks_minor2, minor=True)\n",
    "    #axs[1].set_yticklabels(yticks_minor2, minor=True)\n",
    "\n",
    "    axs[0].tick_params(axis=\"both\", length=5, labelsize=12)\n",
    "    axs[0].tick_params(axis=\"x\", length=3, which=\"minor\", labelsize=9)\n",
    "\n",
    "    axs[1].tick_params(axis=\"both\", length=5, labelsize=12)\n",
    "    axs[1].tick_params(axis=\"x\", length=3, which=\"minor\", labelsize=9)\n",
    "\n",
    "    fig.savefig(f\"..\\\\plots\\\\{experiment}_Wilde_vs_Uppsala.png\", dpi=500)\n",
    "    fig.savefig(f\"..\\\\plots\\\\{experiment}_Wilde_vs_Uppsala.pdf\", dpi=500)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_raw_signal_of_different_WTs(experiment=\"E008\", dimensions=(21/cm_in_inches, 7/cm_in_inches), lw=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
